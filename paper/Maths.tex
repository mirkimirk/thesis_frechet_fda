This section provides mathematical background for fda and the Wasserstein space. Basic
concepts and notation are established that will be used throughout this thesis.

\subsection{Hilbert Space Theory and $\mathcal{L}^2$ Space}
\label{sec:hilbert spaces and l2}
This section will introduce basic mathematical concepts required to explain basic fda
methods. In later sections, these will be used for derivations and proofs. It is mostly
based on \textcite[Chapter~2]{HsingEubank2015}.

\subsubsection{Separable Hilbert Spaces}
\label{sec:separable hilbert spaces}
For many methods in fda to work and be theoretically justified, additional structure on
the set of functions under consideration is assumed. This structure is most often the
$\mathcal{L}^2$ space of square integrable functions. This space is a separable Hilbert
space. To define $\mathcal{L}^2$ and understand its properties as a separable Hilbert
space, we first need the following definitions:
\begin{definition}[Inner Product and Inner Product Space]
    \label{def:inpr}
    Let \( V \) be a real vector space. A function $\inpr{\cdot}{\cdot}_V$ that maps any
    two vectors \( s, t \in V \) to a real number \( \inpr{s}{t}_V \) is called an
    \textit{inner product} if it satisfies the following properties:
    \begin{enumerate}
        \item \textbf{Symmetry}: \( \inpr{s}{t}_V  = \inpr{t}{s}_V \) for all \( s, t \in V \).
        \item \textbf{Linearity in First Argument}: \( \inpr{as + bt}{w}_V = a \inpr{s}{w}_V + b \inpr{t}{w}_V \) for all \( s, t, w \in V \) and \( a, b \in \mathbb{R} \).
        \item \textbf{Positive Definiteness}: \( \inpr{s}{t}_V \geq 0 \) and \( \inpr{t}{t}_V = 0 \) if and only if \( t = 0 \) for all \( s, t \in V \).
    \end{enumerate}
    A real vector space \( V \) equipped with such an inner product is called an
    \textit{inner product space}.
\end{definition}
To generalize all the notions of interest for fda from Eucliean space to possibly
infinite dimensional spaces, we need the following space:
\begin{definition}[Hilbert Space]
    \label{def:hilbert space}
    Let $\mathcal{H}$ be an inner product space, and denote by $\inpr{\cdot}{\cdot}_\mathcal{H}$
    the inner product of this space. Then $\norm{x}_\mathcal{H} = \sqrt{\inpr{x}{x}_\mathcal{H}}$
    is the norm induced by the inner product, and $d_\mathcal{H}(a, b) = \norm{a - b}_\mathcal{H}$
    is the distance induced by the norm. If $\mathcal{H}$ is complete with respect
    to this metric, i.e., if every Cauchy sequence of elements in $\mathcal{H}$ converges
    in $\mathcal{H}$, then $\mathcal{H}$ is called a \textit{Hilbert space}.
\end{definition}
This concept helps generalizing many useful notions from the finite-dimensional
Euclidean space in multivariate analysis to infinite-dimensional function spaces. The norm
$\norm{\cdot}_\mathcal{H}$ measures the size of an element in a Hilbert space $\mathcal{H}$ and makes it
comparable with others. It also induces a metric via
$d_\mathcal{H}(a, b) = \norm{a - b}_\mathcal{H}$ to measure the distance between two
elements in $\mathcal{H}$. The concept of inner product makes it possible to evaluate
the angle between two vectors in $\mathcal{H}$, which is useful for orthogonal
decompositions of this space. Especially the latter will be very important for dimensionality reduction.

To get closer to the goal of dimensionality reduction, we first need some more definitions
to explain the kind of lower-dimensional subspaces we are interested in, and how the
concept of orthogonality plays into that.
\begin{definition}[Closed Span]
    \label{def:closed span}
    Let \( A \) be a subset of elements in a Hilbert
    space \( \mathcal{H} \). The \textit{closed span} of these
    elements is the closure of their linear span with respect to the metric induced by the
    norm, and will be denoted $\overline{\text{Span}(A)}$.
\end{definition}
\begin{definition}[Orthonormal Sequence and Orthonormal Basis]
    \label{def:orthonormal sequence}
    A sequence \( \{ e_n \} \) in a Hilbert space \( \mathcal{H} \) is called an
    \textit{orthonormal sequence} if it satisfies:
    \[
    \inpr{e_i}{e_j}_\mathcal{H} = \delta_{ij}
    \]
    where \( \delta_{ij} \) is the Kronecker delta.

    If $\overline{\text{Span}(\{ e_n \})} =  \mathcal{H}$, then $\{ e_n \}$ is called an
    \textit{orthonormal basis} of $\mathcal{H}$.
\end{definition}
\begin{theorem}[Separability and Orthonormal Basis in Hilbert Spaces]
    \label{thm:separability-orthonormal-basis}
    A Hilbert space \( \mathcal{H} \) is \textit{separable} --- i.e., contains a countable dense
    subset --- if and only if it has an orthonormal basis.
\end{theorem}
Later we will be approximating elements of a Hilbert space by a combination of a finite
number of its elements. So an orthonormal basis assures us, that there is this dense
subset that can get arbitrarily close to any element in the space. To find the most
efficient subspace (in the sense of giving the best approximation of any element for a
given number of dimensions of the subspace), we want to exploit some more structure of
the space $\mathcal{L}^2$ that we will be interested in. This structure
is given by its separability and Hilbert space properties.

In a separable Hilbert space, the existence of a countable orthonormal basis allows us to
approximate any element \( f \) in the space arbitrarily closely by finite linear combinations
of the basis elements. This foundational idea is critical for methods like Functional Principal
Component Analysis, where we approximate complex functions by projecting them onto a finite
subset of this basis. This will be explained further in Section~\ref{sec:fpca}.

\subsubsection{The Space $\mathcal{L}^2([a, b])$}
\label{sec:l2 space}
In the context of fda, it is assumed that the functional data live in
the space of square integrable functions $\mathcal{L}^2([a,b])$, with common
support $[a,b]$.
\begin{definition}[Space of square integrable functions $\mathcal{L}^2$]
    Let $\mathcal{A} \subseteq \mathbb{R}$ be compact. The space \( \mathcal{L}^2(\mathcal{A}) \)
    consists of all functions \( f: \mathcal{A} \to \mathbb{R} \) such that
    \[
    \int_{\mathcal{A}} (f(x))^2 \, dx < \infty
    \]
    Its norm, inner product, and distance are denoted by $\norm{\cdot}_2$,
    $\inpr{\cdot}{\cdot}_2$, and $d_2(\cdot, \cdot)$, respectively.
\end{definition}
The concepts introduced here will be needed for an explanation of fda methods in Section~\ref{sec:fda}.
Before that, we will turn to introducing basic statistical notions in function spaces
and the Wasserstein geometry that will be important later.

\subsection{Statistics in Functional Spaces}
\label{sec:stat_foundations}
Here, we will introduce distribution functions, the concept of Fréchet means as generalizations
of the usual expected values for arbitrary metric spaces, and the Wasserstein geometry
that we will deal with in the Fréchet regression context.

\subsubsection{Preliminaries and Definitions}
\label{sec:definitions}
A very basic operation we will use throughout this work is switching between different functions
that characterize a distribution. We will deal with the following four important classes
of functions that characterize a distribution:
\begin{definition}[pdfs, cdfs, qfs, and qdfs]
    \label{def:distributionfuncs}
    Let \(\mathbb{R}^+\) be the set of nonnegative reals.
    \begin{enumerate}
        \item The \textit{probability density function (pdf)} is a function
        \( f: \mathbb{R} \to \mathbb{R}^+ \) such that \( \int_{\mathbb{R}} f(x) \, dx = 1 \)
        and \( f(x) \geq 0 \) for all \( x \in \mathbb{R} \).

        \item The \textit{cumulative cistribution function (cdf)} is given by
        \( F(x) \coloneqq \int_{-\infty}^{x} f(t) \, dt \), where \( f \) is the pdf.
        Note that \( F\) is non-decreasing and right-continuous,
        with \( \lim_{{x \to -\infty}} F(x) = 0 \) and \( \lim_{{x \to \infty}} F(x) = 1 \).

        \item The inverse of \(F\) is called the \textit{quantile function (qf)},
        and is denoted by \(Q\). It is given by \( Q(u) \coloneqq F^{-1}(u) =
        \inf \{ x \in \mathbb{R} \mid F(x) \geq u \} \) for \( u \in [0, 1] \).

        \item The derivative of \(Q(u)\) w.r.t. \(u\) is called the \textit{quantile
        density function (qdf)}, and is denoted by \(q\). It is given by
        \(q(u) \coloneqq \frac{d}{du} Q(u)\) for \( u \in [0, 1] \).
    \end{enumerate}
\end{definition}
For clarity, we will use the arguments $x$ and $z$ for pdfs and cdfs, and the arguments
$u$ and $v$ for qfs and qdfs. Since we will later only look at densities from continuous
distributions on their support (i.e., where $f > 0$), we will in the following assume
that $F$ is strictly increasing and thus its inverse $Q$ to be well-defined.
The following relation between the pdf and qdf will be useful later when computing
transformations of the density functions \parencite[cf.][]{Jones1992}:
\begin{lemma}
\label{lemma:f eq inverse qdf}
    Let \(f\) be a pdf, \(F\) the corresponding cdf, \(Q\) the corresponding qf, and
    \(q\) the corresponding qdf. Then it holds that
    \begin{equation}
    \label{eq:qdfinversef}
        {q}(u) = \frac{1}{{f}({Q}(u))},
    \end{equation}
    and
    \begin{equation}
    \label{eq:finverseqdf}
        {f}(x) = \frac{1}{{q}({F}(x))},
    \end{equation}
\end{lemma}
\begin{proof}
    We will show that \eqref{eq:qdfinversef} holds, \eqref{eq:finverseqdf} follows
    analogously. Note that \( q = (F^{-1})' \), so we can use the inverse function
    rule to characterize \(q \) with the inverse $Q = F^{-1}$ and derivative $f = F'$
    of \( F \):
    \begin{equation}
    \label{eq:proof_qdfinversef}
        (F^{-1})'(u) = \frac{1}{{F'}({F^{-1}}(u))} = \frac{1}{{f}({Q}(u))}
    \end{equation}
\end{proof}
Because of the relation in \eqref{eq:qdfinversef}, the qdf is sometimes called the
"sparsity function" \parencite[cf.][]{Tukey1965}.

The distributions that are of interest to us are given in the following definition:
\begin{definition}
    \label{def:spaceD}
    Denote by \( \mathcal{G} \) the space of absolutely continuous probability
    distributions \( G \) supported on their own compact interval \( [a_i, b_i] \)
    for some \( a_i, b_i \in \mathbb{R} \) with \( a_i < b_i \), and having a finite
    second moment. A distribution \( G \) is in \( \mathcal{G} \) if it admits a
    density function \( f \) such that
    \[
        \int_{a_i}^{b_i} x^2 \, dG(x) = \int_{a_i}^{b_i} x^2 f(x) \, dx < \infty.
    \]
\end{definition}
Note that the distributions need not share any point in their support.

Lastly, we denote by $\mathcal{F}$ the density-valued stochastic process producing our
sample.

\subsubsection{Fréchet Mean and Variance}
\label{sec:f_mean}
Let $(\Omega, d)$ be a metric space and $Y$ a random variable that takes values in
$\Omega$. We define the Fréchet mean by
\begin{equation}
    Y_\oplus = \argmin_{\omega \in \Omega} \mathbb{E} \left[ d^2(Y, \omega) \right]
    \label{eq:fmean}
\end{equation}
This concept generalizes the estimation of mean values to any metric space. If $d$ is
the Euclidean distance $d_E$, this mean reduces to the usual mean $\mathbb{E}(Y)$. Accordingly,
we define Fréchet variance as
\begin{equation}
    V_\oplus = \mathbb{E} \left[ d^2(Y, Y_\oplus) \right]
    \label{eq:fvar}
\end{equation}
In Section~\ref{sec:cond_fmean}, this notion will be generalized to that of a conditional
Fréchet mean, to make use in a regression context and define Fréchet regression.The metric
we will be using throughout is the 2-Wasserstein distance defined in the following section.

\subsubsection{Wasserstein Distance and Its Properties}
\label{sec:wasserstein_distance}
The metric space that we are interested in is the space $\mathcal{G}$ equipped with the
Wasserstein-2 metric, defined in the following:
\begin{definition}[2-Wasserstein Distance]
    \label{def:wasserstein_2_dist}
    For \( f, g \in \mathcal{G} \), consider the collection \( \mathcal{M}_{f,g} \) of maps \( M \)
    such that if \( U \sim f \) and \( M \in \mathcal{M}_{f,g} \), then \( M(U) \sim g \).
    The 2-Wasserstein distance between \( f \) and \( g \) is defined as:
    \[
    d_W(f, g) = \left( \inf_{M \in \mathcal{M}_{f,g}} \int (M(u) - u)^2 f(u) \, du \right)^{1/2}
    \]
\end{definition}
Before turning to the consequences for the Fréchet mean and its computation in the next section,
we discuss some properties and characterizations of this distance.

While in general the Wasserstein distance is not simple to evaluate, in our case it can
be shown that it is equal to the quantile distance, given by
\[
d_Q(f, g)^2 \coloneqq d_Q(Q_f, Q_g)^2 = \int_{0}^{1} (Q_f(t) - Q_g(t))^2 dt,
\]
where $Q_f, Q_g$ are the quantile functions corresponding to the densities $f, g \in \mathcal{G}$ space.

\begin{lemma}
    \label{lemma:dqeqdw}
    $d_Q = d_W$
\end{lemma}
See \textcite{PetersenLiuDivani2021} and the supplement of \textcite{PetersenMüller2016}
for proofs.

Setting the distance function to be $d_W(f, g)$ in \ref{eq:fmean}, we yield the Wasserstein-Fréchet mean.
See \textcite{PetersenLiuDivani2021} for a definition.

% \begin{lemma}
%     The Wasserstein-Fréchet mean estimator is given by
%     \begin{equation}
%         \hat{f}_\oplus(x) = \frac{1}{\hat{q}_\oplus(\hat{F}_\oplus(x))},
%     \end{equation}
%     with $\hat{q}_\oplus = \frac{1}{n} \sum_{1}^{n} q_i$.
% \end{lemma}
% \begin{proof}
%     By Lemma~\ref{lemma:dqeqdw}, we can substitute the quantile distance for the
%     Wasserstein distance in the computation of the Fréchet mean. We set
%     $Q_\oplus(t) = \mathbb{E}[Q(t)]$ because $\mathbb{E}[Q(t)]$ is the minimizer of
%     \begin{equation}
%     \label{eq:wf_mean}
%         \begin{aligned}
%             \mathbb{E}[d_w^2(f_i, f_\oplus)]	& =
%             \mathbb{E}\left[\int_{0}^{1}(F_i^{-1}(t) - F_\oplus^{-1}(t))^2 \,dt\right] \\
%                                     & =
%             \mathbb{E}\left[\int_{0}^{1}(Q_i(t) - Q_\oplus(t))^2 \,dt\right],
%         \end{aligned}
%     \end{equation}
%     see \citet[Chapter~3.1.4]{PanaretosZemel2020}. To compute the corresponding density
%     function, use inverse function rule to get
%     $f_\oplus(x) = \frac{1}{q_\oplus(F_\oplus(x))}$, which shows that it suffices to
%     estimate $q_\oplus$. Remember that
%     $q_\oplus = \frac{\mathrm{d}Q_\oplus}{\mathrm{d}t}$. We can pass the differentiation inside the expectation to see
%     $q_\oplus = \mathbb{E}\left[\frac{\mathrm{d}Q}{\mathrm{d}t}\right]$, which by analogy
%     principle suggests to average the sample observed (or previously estimated)
%     quantile densities $q_i$ to obtain an estimator $\hat{q}_\oplus$ for $q_\oplus$.
% \end{proof}
