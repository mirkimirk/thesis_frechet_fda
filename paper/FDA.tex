This section provides mathematical background for fda, and establishes basic concepts
and notation that will be used throughout this text.

\subsection{Basic Definitions and Notation}
\label{sec:basics}
A very basic operation we will use in this text is switching between different functions
that characterize a distribution. We will deal with the following four important classes
of functions that characterize a distribution:

\begin{definition}[pdfs, cdfs, qfs, and qdfs]
    \label{def:distributionfuncs}
    Let \(\mathbb{R}^+\) be the set of nonnegative reals.
    \begin{enumerate}
        \item The \textit{probability density function (pdf)} is a function
        \( f: \mathbb{R} \to \mathbb{R}^+ \) such that \( \int_{\mathbb{R}} f(x) \, dx = 1 \)
        and \( f(x) \geq 0 \) for all \( x \in \mathbb{R} \).

        \item The \textit{cumulative cistribution function (cdf)} is given by
        \( F(x) \coloneqq \int_{-\infty}^{x} f(t) \, dt \), where \( f \) is the pdf.
        Note that \( F\) is non-decreasing and right-continuous,
        with \( \lim_{{x \to -\infty}} F(x) = 0 \) and \( \lim_{{x \to \infty}} F(x) = 1 \).

        \item The inverse of \(F\) is called the \textit{quantile function (qf)},
        and is denoted by \(Q\). It is given by \( Q(u) \coloneqq F^{-1}(u) =
        \inf \{ x \in \mathbb{R} \mid F(x) \geq u \} \) for \( u \in [0, 1] \).

        \item The derivative of \(Q(u)\) w.r.t. \(u\) is called the \textit{quantile
        density function (qdf)}, and is denoted by \(q\). It is given by
        \(q(u) \coloneqq \frac{d}{du} Q(u)\) for \( u \in [0, 1] \).
    \end{enumerate}
\end{definition}

For clarity, we will use the arguments $x$ and $z$ for pdfs and cdfs, and the arguments
$u$ and $v$ for qfs and qdfs. Since we will later only look at densities from continuous
distributions on their support (i.e., where $f > 0$), we will in the following assume
that $F$ is strictly increasing and thus its inverse $Q$ to be well-defined.
The following relation between the pdf and qdf will be useful later when computing
transformations of the density functions \parencite[cf.][]{Jones1992}:

\begin{lemma}
\label{lemma:f eq inverse qdf}
    Let \(f\) be a pdf, \(F\) the corresponding cdf, \(Q\) the corresponding qf, and
    \(q\) the corresponding qdf. Then it holds that
    \begin{equation}
    \label{eq:qdfinversef}
        {q}(u) = \frac{1}{{f}({Q}(u))},
    \end{equation}
    and
    \begin{equation}
    \label{eq:finverseqdf}
        {f}(x) = \frac{1}{{q}({F}(x))},
    \end{equation}
\end{lemma}
\begin{proof}
    We will show that \eqref{eq:qdfinversef} holds, \eqref{eq:finverseqdf} follows
    analogously. Note that \( q = (F^{-1})' \), so we can use the inverse function
    rule to characterize \(q \) with the inverse $Q = F^{-1}$ and derivative $f = F'$
    of \( F \):
    \begin{equation}
    \label{eq:proof_qdfinversef}
        (F^{-1})'(u) = \frac{1}{{F'}({F^{-1}}(u))} = \frac{1}{{f}({Q}(u))}
    \end{equation}
\end{proof}

Because of the relation in \eqref{eq:qdfinversef}, the qdf is sometimes called the
"sparsity function" \parencite[cf.][]{Tukey1965}.
($\Omega$ denotes the space of random objects
we are interested in. $\mathcal{G}$ will denote the "set of distributions", i.e., the set containing
any class of functions that can represent a distribution.)

Following \textcite{PetersenZhangKokoszka2022}, we denote with
\begin{equation}
\label{eq:density_set}
    \mathcal{D} \subseteq \left\{ f : f(x) \geq 0, \int_{\mathbb{R}} f(x) \, dx = 1 \right\}
\end{equation}
the set of probability density functions (pdfs) that are of interest to us.\footnote{Note that
this in particular means that $\mathcal{D}$ can contain densities with different supports.
This will be significant later on when we perform transformations on these densities.}
$\Omega$ will be of use when generally explaining fda methods and theory, as well as
the concept of Fréchet regression. In the applications, we will be focussed on either
$\Omega = \mathcal{G}$ or $\Omega = \mathcal{D}$

\subsection{Theorems from Hilbert Space Theory}
\label{sec:hilbert spaces}
This section will introduce basic mathematical concepts needed for functional data
analysis. In later sections, these will be used for derivations and proofs.

For many methods in fda to work and be theoretically justified, additional structure on
the set of functions under consideration is assumed. This structure is most often the
$\mathcal{L}^2$ space of square integrable functions. This space is a separable Hilbert
space. To define $\mathcal{L}^2$ and the separable Hilbert space, we first need the
following definitions:

\begin{definition}[Inner Product and Inner Product Space]
    Let \( V \) be a real vector space. A function $\inpr{\cdot}{\cdot}_V$ that maps any
    two vectors \( s, t \in V \) to a real number \( \inpr{s}{t}_V \) is called an
    \textit{inner product} if it satisfies the following properties:
    \begin{enumerate}
        \item \textbf{Symmetry}: \( \inpr{s}{t}_V  = \inpr{t}{s}_V \) for all \( s, t \in V \).
        \item \textbf{Linearity in First Argument}: \( \inpr{as + bt}{w}_V = a \inpr{s}{w}_V + b \inpr{t}{w}_V \) for all \( s, t, w \in V \) and \( a, b \in \mathbb{R} \).
        \item \textbf{Positive Definiteness}: \( \inpr{s}{t}_V \geq 0 \) and \( \inpr{t}{t}_V = 0 \) if and only if \( t = 0 \) for all \( s, t \in V \).
    \end{enumerate}
    A real vector space \( V \) equipped with such an inner product is called an
    \textit{inner product space}.
\end{definition}

For fda, we need the following special case of an inner product space:
\begin{definition}[Hilbert Space]
    Let $\mathcal{H}$ be an inner product space, and denote by $\inpr{\cdot}{\cdot}_\mathcal{H}$
    the inner product of this space. Then $\norm{x}_\mathcal{H} = \sqrt{\inpr{x}{x}_\mathcal{H}}$
    is the norm induced by the inner product, and $d_\mathcal{H}(a, b) = \norm{a - b}_\mathcal{H}$
    is the distance induced by the norm. If $\mathcal{H}$ is complete with respect
    to this metric, i.e., if every Cauchy sequence of elements in $\mathcal{H}$ converges
    in $\mathcal{H}$, then $\mathcal{H}$ is called a \textit{Hilbert space}.
\end{definition}
This concept helps generalizing many useful notions from the finite-dimensional
Euclidean space in multivariate analysis to infinite-dimensional function spaces. The norm
$\norm{\cdot}_\mathcal{H}$ measures the size of an element in $\mathcal{H}$ and makes it
comparable with others. It also induces a metric via
$d_\mathcal{H}(a, b) = \norm{a - b}_\mathcal{H}$ to measure the distance between two
elements in $\mathcal{H}$. The concept of inner product makes it possible to evaluate
the angle between two vectors in $\mathcal{H}$, which is useful for defining projections
and for orthogonal decompositions of this space. Especially the latter will be very
important for dimensionality reduction.

\begin{definition}[Space of square integrable function $\mathcal{L}^2$]
    Let $\mathcal{A} \subseteq \mathbb{R}$ be compact. The space \( \mathcal{L}^2(\mathcal{A}) \)
    consists of all functions \( f: \mathcal{A} \to \mathbb{R} \) such that
    \[
    \int_{\mathcal{A}} (f(x))^2 \, dx < \infty
    \]
\end{definition}

A Hilbert Space is this and that....

An operator is a functional of elements in this space. It serves as a generalization of
a matrix and will play the same conceptual role in the FDA analogues to mv methods
that matrices do in them.

This is the covariance operator... it is symmetric and positive semi-definite, so
a Hilbert-Schmidt operator (\textcite{WangChiouMüller2016} say because of the integral form,
its a trace class, so compact Hilbert-Schmidt operator). It allows for the spectral
decomposition (in terms of eigenfunctions and eigenvalues). The space of Hilbert-Schmidt
operators is itself a separable Hilbert Space.

\subsubsection{Riesz Representation Theorem}
\label{sec:riesz}

\subsubsection{Mercer's Theorem and Karhunen-Loève Decomposition}
\label{sec:mercer and kh}

\subsection{$L^2([a, b])$ Space}
\label{sec:l2 space}
In the context of FDA, it is assumed that the functional data live in
the space of square integrable functions $L^2([a,b])$, with common
support $[a,b]$. This space is a separable Hilbert space, i.e., a complete inner
product space, equipped with the norm induced by the inner product. Separability means
its elements can arbitrarily well be approximated by elements of a dense subset of the
space, and that this subset is not unhandably large.
This allows to generalize notions of distance (as the norm induces a
metric), magnitude (given by the norm), and orthogonality (defined by
$\inpr{x}{y} = 0$, with $\inpr{\cdot}{\cdot}$ being the inner product) of elements
in the space from the Euclidean space, in which we usually work, to more
abstract and potentially infinite dimensional spaces, such as function
spaces.

\subsection{Functional Principal Component Analysis}
\label{sec:fpca}
The most popular method for describing structure in our functional data is Functional
Principal Component analysis (FPCA). This is an analogue to Principal Component Analysis
(PCA) from multivariate statistics in the case of infinite dimensions. It builds on the
Karhunen-Loève decomposition (described in the section \ref{sec:mercer and kh}) to
recover functions that describe the main modes of variation, in descending order.

We computed the discretized grid as described in \citet[Chapter~8.4.1]{RamsaySilverman2005}. Another analogous way
is described in \textcite{KneipUtikal2001}. (\textcite{Delicado2011} SUMMARIZES THIS.)
