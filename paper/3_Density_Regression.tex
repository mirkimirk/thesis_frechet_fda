Blablabla
Distributions can be characterized by f, F, Q, and q, among others. Of these, the least 
constraints are on q (sometimes called "sparsity", see TUKER 1962). These vanish if we
take the logarithm (SEE KOKOSZKA ET AL 2019). Since our DGP generates densities on
different supports, we use KOKOSZKA ET AL (2019)s modification on the log quantile-density
transformation, i.e., our transformation yields a tuple ($psi(f), Q(s_0)$), with
$psi(f)$ being the transformation of f as defined by {PetersenMÃ¼ller2016}, and
$Q(s_0)$ being the start of the density support ($s_0 > 0$ is fulfilled, since the support
of $Q$ is chosen away from zero for numerical reasons anyway). For the predicted $f_i$ hats,
in the inverse transformation the $s_0i$ from the corresponding observation $f_i$ was
assigned as estimate $s_0i$hat.

(Transformation approach regression: underestimating mu and sigma for lower x,
overestimating for higher x?) \citet{PetersenLiuDivani2021}