This section gives a brief introduction to some foundations of FDA as well as basic
methods in the context of functional responses. The relation and applicability to the
case of densities as responses is discussed.

\subsection{Basic Concepts and Notation}
\label{sec:basics}
Throughout this text, the following notation and concepts will be used.

\begin{definition}[$L^2$]
    The space \( L^2(\Omega) \) consists of all functions \( f: \Omega \to \mathbb{R} \) such that
    \[
    \int_{\Omega} |f(x)|^2 \, dx < \infty
    \]
\end{definition}

\subsection{Hilbert Space Theory and Linear Operators}
\label{sec:hilbert spaces}
This section will introduce basic mathematical concepts needed for functional data
analysis before they will be explained in their relation to specific methods in the
following sections.

A Hilbert Space is this and that....

An operator is a functional of elements in this space. It serves as a generalization of
a matrix and will play the same conceptual role in the FDA analogues to mv methods
that matrices do in them.

This is the covariance operator... it is symmetric and positive semi-definite, so
a Hilbert-Schmidt operator (\textcite{WangChiouMüller2016} say because of the integral form,
its a trace class, so compact Hilbert-Schmidt operator). It allows for the spectral
decomposition (in terms of eigenfunctions and eigenvalues). The space of Hilbert-Schmidt
operators is itself a separable Hilbert Space.

\subsubsection{Riesz Representation Theorem}
\label{sec:riesz}

\subsubsection{Mercer's Theorem and Karhunen-Loève Decomposition}
\label{sec:mercer and kh}

\subsection{$L^2([a, b])$ Space}
\label{sec:l2 space}
In the context of FDA, it is assumed that the functional data live in
the space of square integrable functions $L^2([a,b])$, with common
support $[a,b]$. This space is a separable Hilbert space, i.e., a complete inner
product space, equipped with the norm induced by the inner product. Separability means
its elements can arbitrarily well be approximated by elements of a dense subset of the
space, and that this subset is not unhandably large.
This allows to generalize notions of distance (as the norm induces a
metric), magnitude (given by the norm), and orthogonality (defined by
$\inpr{x}{y} = 0$, with $\inpr{\cdot}{\cdot}$ being the inner product) of elements
in the space from the Euclidean space, in which we usually work, to more
abstract and potentially infinite dimensional spaces, such as function
spaces.

\subsection{Functional Principal Component Analysis}
\label{sec:fpca}
The most popular method for describing structure in our functional data is Functional
Principal Component analysis (FPCA). This is an analogue to Principal Component Analysis
(PCA) from multivariate statistics in the case of infinite dimensions. It builds on the
Karhunen-Loève decomposition (described in the \ref{sec:mercer and kh}) to recover functions
that describe the main modes of variation, in descending order.

We computed the discretized grid as described in \citet[Chapter~8.4.1]{RamsaySilverman2005}. Another analogous way
is described in KNEIP AND UTIKAL.
