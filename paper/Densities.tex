In this chapter we are going to examine the case of density regression, i.e., a
regression model with densities as responses. The densities that are of interest to
us are given in the following definition:

\begin{definition}
    Denote by $\mathcal{D}$ the space of continuous densities $f$, where each
    $f$ is supported on its own compact interval $[a_i, b_i]$ for some $a_i, b_i \in
    \mathbb{R}$ with $a_i < b_i$.
\end{definition}

\subsection{Approaches}
\label{sec:approaches}
Here, we will briefly outline different interpretations of the density space, and the
approaches that follow, to motivate our choice of the transformation method from
\textcites{PetersenMüller2016}{KokoszkaEtAl2019}.

\subsubsection{Subset of Hilbert Space}
\label{sec:l2_interpretation}
Since $\mathcal{D} \subseteq L^2(\mathbb{R})$, one can choose to interpret the densities
as elements of the Hilbert space $\mathcal{L}^2$ and directly apply fda methods on the densities
\parencite[see e.g.][]{KneipUtikal2001}. Then one could handle the regression problem by
applying the functional response regression method first described by \textcite{Faraway1997}.
However, as $\mathcal{D}$ is not a linear subspace of $\mathcal{L}^2$, it is not guaranteed that
we yield results within $\mathcal{D}$ --- it only means we will yield results within $\mathcal{L}^2$,
so our estimates, modes of variation etc. are not guaranteed to be themselves densities.
Alternative characterizations of the space $\mathcal{D}$ have been explored to tackle
this problem, of which
we will focus in this paper on the one of a nonlinear space \ref{sec:transformation_interpretation}
and the \textit{Wasserstein space}. The latter interpretation will be explored in
Section~\ref{sec:fréchet regression}.

\subsubsection{Transformation Method}
\label{sec:transformation_interpretation}
Instead of the approach above, one can recognize the nonlinear character of the density
space and accomodate the use of fda methods such that it is respected. Several authors
have proposed a prior transformation of the densities into a linear space
\parencites[e.g.][]{Hron2016}[][]{PetersenMüller2016} where one can then work with the
usual fda methods on the transformed densities, and then later transform the results
back to density space for interpretation and visualization purposes.

Following the argument in \textcite{KokoszkaEtAl2019}, we use the method first outlined
in \textcite{PetersenMüller2016} and use the log quantile density (LQD) transformation
$\psi : \nobreak \mathcal{D} \to \mathcal{L}^2([0,1])$, defined by
\begin{equation}
    \label{eq:lqd_definition}
    \psi (f)(u) = \log q(u) = -\log f(Q(u)), \quad u \in [0,1],
\end{equation}
to transform the pdfs from the constrained space $\mathcal{D}$ into the "unconstrained"
space $\mathcal{L}^2([0,1])$.\footnote{We extend our usage of the argument $u$ to this
function because this transformed density is by definition on the same support as are
qfs and qdfs.} The intuition is that, from the functions given in
Definition~\ref{def:distributionfuncs}, the qf and qdf have the least constraints ---
only requiring that $q = \nobreak Q' \geq \nobreak 0$. Thus by working with the logarithm
of the qdf, one deals with a function that has no more constraints \parencite[cf.][]{KokoszkaEtAl2019}.

However, since the data generating process (DGP) that we will use in our simulation study
produces densities that are on different supports, we violate the assumption of common
compact support of the densities in the density sample that \textcite{PetersenMüller2016}
need.\footnote{We also violate this assumption because of our method of dealing with
numerical issues, see Appendix~\ref{sec:numerics}.} This leads to problems since the information
about location of support is lost when transforming a pdf to a qdf.\footnote{Sligthly
paraphrasing \textcite{KokoszkaEtAl2019}, the transformation from $f$ with quantile
density $q$ to its LQD is not invertible, since for any constant $c$, the quantile density of
$f(\cdot - c)$, the density $f$ shifted by $c$, is also $q$.}
Hence we use the slightly modified transformation and inverse transformation described
by \textcite{KokoszkaEtAl2019}. In this paper a very general transformation method is
described for the case of different supports. Denote by $\mathcal{A}_i$ the support of
the density $f_i$. For generality, we assume
\begin{equation*}
    \exists i, j \, : \, \mathcal{A}_i \cap \mathcal{A}_j = \emptyset,
\end{equation*}
i.e., we do not want to assume a common "anchor" point in the supports of the $f_i$. So
we use the following most general transformation proposed by the authors:
\begin{equation}
    T : \mathcal{D} \to \mathcal{L}^2([0,1]) \times \mathcal{A}, \text{ given by } T(f) = (\psi(f), Q(u_0)),
\end{equation}
where $\mathcal{A}$ is the support of density $f$, $Q$ is the corresponding quantile function,
and $u_0 \in [0,1]$ is a fixed percentile level for which we save the value of the support
of $f$. Since we deal with a compact support, we can choose the boundary value
0\footnote{\textcite{KokoszkaEtAl2019} excluded 0 and 1 because they allowed for the case
of unbounded support, which could yield $Q(0)$ or $Q(1)$ to be infinity, so useless for
the transformation method.}, which is most intuitive: we save the left bound of the
support of $f$, and for the inverse transformation we add that bound when computing
the quantile function $Q$ from the LQD. The transformation method is described in the
following two algorithms:

\begin{algorithm}
    \caption{Forward transformation}
    \label{alg:forward}
    \begin{algorithmic}[1]
    \Require Data pairs \( (x_l, f(x_l)), l = 0, \ldots, L \)
    \Ensure Data pairs \( (u_l, \psi(f)(u_l)), l = 0, \ldots, L \) and \( c \)
    \For{\( l = 1, \ldots, L \)}
        \State Compute \( F(x_l) = \int_{x_0}^{x_l} f(x) \, dx \) by numerical integration of the pairs \( (x_l, f(x_l)) \)
        \State Define \( u_l = F(x_l) \) so that \( Q(u_l) = x_l \)
        \State Compute \( \psi(f)(u_l) = -\ln(f(x_l)) \)
        \State Find \( j \) such that \( x_j = 0 \) and set \( c = Q(0) \)
    \EndFor
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}
    \caption{Backward transformation}
    \label{alg:backward}
    \begin{algorithmic}[1]
    \Require Data pairs \( (u_l, g(u_l)), l = 0, \ldots, L \) and \( c \)
    \Ensure Data pairs \( (x_l, f(x_l)), l = 0, \ldots, L \)
    \For{\( l = 1, \ldots, L \)}
        \State Compute \( Q(u_l) = c + \int_{0}^{u_l} \exp\{g(s)\} \, ds \) by numerical integration of the pairs \( (u_l, \exp\{g(u_l)\}) \)
        \State Define \( x_l = Q(u_l) \) so that \( F(x_l) = u_l \)
        \State Compute \( f(x_l) = \exp \{-g(u_l)\} \)
    \EndFor
    \end{algorithmic}
\end{algorithm}

Here, $g$ is some function from the transform space $\mathcal{L}^2([0,1])$,
resulting from some calculations we performed with the transformed densities. For
the computation of the inverse, we need to supply a value of $c$ for this function $g$.
In our application we will deal with two kinds of functions $g$: estimated LQDs from
our functional regression for some grid of predictor values; and a mean LQD computed from
the LQD sample. For the former, we estimated $c$ by defining a function $c(p)$, which
assigns each $c$ to the predictor value $p$ with which it was observed, and calculates $\hat{c}$
with linear interpolation. For the mean LQD, we computed the mean of the $c$ in the sample
and used that as an estimator. These $\hat{c}$ are supposed to estimate the left bound
of the support of the resulting density when performing the inverse transform $T^{-1}$
described in \ref{alg:backward}.

\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \resizebox{\linewidth}{!}{\input{../bld/figures/fda/naive_trunc_rep_vs_orig.pgf}}
        \caption[Truncated representation --- naive]{Naive method.}
        \label{fig:naive_trunc_rep}
    \end{subfigure}
    \hfill % add some horizontal spacing
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \resizebox{\linewidth}{!}{\input{../bld/figures/fda/trunc_rep_vs_orig.pgf}}
        \caption[Truncated representation --- LQD method]{LQD method.}
        \label{fig:trunc_rep}
    \end{subfigure}
    \caption[Truncated Karhunen-Loève representations]{Truncated Karhunen-Loève
    representations of a sample density. The blue curves result from using one principal
    component, the orange curves results from using two components.}
    \label{fig:trunc_reps}
\end{figure}

As an illustration for the difference in quality of the LQD method to the naive method,
Figure~\ref{fig:trunc_reps} shows the truncated Karhunen-Loève representations of a
sample density with both methods. Further illustrations, as well as the description of the
DGP behind this density, are given in Appendix~\ref{sec:illustration_lqd}.

In the following section, we will explain a functional regression model based on the
LQD method.

\subsection{Functional Regression Model}
\label{sec:func_reg_model}
The transformation approach employed by \textcite{TalskaEtAl2018} can be considered a
special case of the general transformation approach explained by \textcite{PetersenMüller2016}
\parencite[cf.][]{PetersenZhangKokoszka2022}. So we follow their modelling of a functional
response model for our LQD transformation method.

In the following, $n$ denotes the sample size and $r$ denotes the number of scalar
predictors $p$. Our original regression model of interest is
\begin{equation}
    \label{eq:dens_reg_model}
    f_i(t) = \beta_0(t) + \sum_{j=1}^{r} p_{ij} \beta_j(t) + \varepsilon_i(t), \quad i = 1, \ldots, n,
\end{equation}
where $f_i$ is the $i$-th observed density, $p_{ij}$ the $i$-th observed value of the
$j$-th predictor, $\epsilon_i$ being an i.i.d. functional random error in $\mathcal{L}^2([0,1])$
with mean zero, and lastly $\beta_j$ being the $j$-th functional regression coefficient of the $j$-th
predictor $p_j$. This can be written simpler in matrix notation as
\begin{equation}
    \label{eq:dens_reg_model_matrix}
    \mathbf{f}(t) = \mathbf{P} \boldsymbol{\beta}(t) + \boldsymbol{\varepsilon}(t).
\end{equation}
where $\mathbf{f}$ is the $n$-dimensional vector of observed pdfs, $\mathbf{P}$ is an $n \times (r+1)$
matrix containing the scalar predictor values $p_{ij}$ (and the first column containing only
ones), $\boldsymbol{\beta}$ is the $n$-dimensional vector of functional regression coefficients,
and $\boldsymbol{\varepsilon}$ is the $n$-dimensional vector of functional random errors.

Applying the LQD transformation \eqref{eq:lqd_definition} to \eqref{eq:dens_reg_model}
yields
\begin{equation}
    \label{eq:lqd_reg_model}
    \psi(f_i)(t) = \psi(\beta_0)(t) + \sum_{j=1}^{r} p_{ij} \psi(\beta_j)(t) + \psi(\varepsilon_i)(t), \quad i = 1, \ldots, n.
\end{equation}
Defining $\tilde{\beta}_j \coloneqq \psi(\beta_j)$, we estimate $\tilde{\beta}_j$ by
minimizing the sum of squared errors:


(Transformation approach regression: underestimating mu and sigma for lower x,
overestimating for higher x?) \textcite{PetersenLiuDivani2021}

Figure \ref{fig:trunc_reps} shows truncated Karhunen-Loève representations of a given
pdf for the respective methods. The blue curve is for using only one component, the
orange curve for using the two most important components. The black curve is the pdf
that is to be represented. One can clearly see that the representations based on the LQD
method produce much better fitting curves than the naive method. Also, the curves barely
differ when including more components in the naive case.

\begin{figure}[h]
    \centering
    % \input{../bld/figures/frechet/betas.pgf}
    \resizebox{1\textwidth}{!}{\input{../bld/figures/frechet/betas.pgf}}
    \caption[Estimated betas in LQD functional regression]{Plot of estimated betas from
    functional regression with LQD method.}
    \label{fig:betas}
\end{figure}

\begin{figure}[h]
    \centering
    % \input{../bld/figures/frechet/beta0vsmean.pgf}
    \resizebox{1\textwidth}{!}{\input{../bld/figures/frechet/beta0vsmean.pgf}}
    \caption[Equality of mean LQD function and $\beta_0$]{Plot that shows the equality
    of the mean LQD function and $\beta_0$.}
    \label{fig:beta0vsmean}
\end{figure}

\begin{figure}[h]
    \centering
    % \input{../bld/figures/frechet/func_est_vs_true.pgf}
    \resizebox{1\textwidth}{!}{\input{../bld/figures/frechet/func_est_vs_true.pgf}}
    \caption[Comparison: estimated vs. true densities --- LQD]{Estimated densities from
    functional regression with LQD method plotted against the true densities for some
    selected predictor values.}
    \label{fig:func_est_vs_true}
\end{figure}
