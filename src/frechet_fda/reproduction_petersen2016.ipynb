{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from frechet_fda.data_generation_tools import (\n",
    "    gen_params_scenario_one,\n",
    "    gen_truncnorm_pdf_points,\n",
    "    make_estimated_truncnorm_pdf,\n",
    "    make_truncnorm_pdf,\n",
    ")\n",
    "from frechet_fda.distribution_tools import (\n",
    "    frechet_mean,\n",
    "    get_optimal_range,\n",
    "    inverse_log_qd_transform,\n",
    "    log_qd_transform,\n",
    "    make_function_objects,\n",
    "    mean_func,\n",
    ")\n",
    "from frechet_fda.fda_tools import (\n",
    "    compute_centered_data,\n",
    "    compute_cov_function,\n",
    "    compute_principal_components,\n",
    "    gen_qdtransformation_pcs,\n",
    "    k_optimal,\n",
    "    karhunen_loeve,\n",
    "    mode_of_variation,\n",
    "    total_frechet_variance,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "n = 200\n",
    "grid_size = 10000\n",
    "trunc = 3\n",
    "mus, sigmas = gen_params_scenario_one(n)\n",
    "# Sort sigmas, because when summing Function instances something goes wrong otherwise\n",
    "sigmas.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate pdfs within truncation points\n",
    "pdfs = make_truncnorm_pdf(\n",
    "    -trunc, trunc, mus, sigmas, grid_size=grid_size, warn_irregular_densities=False,\n",
    ")\n",
    "# Make Function class objects\n",
    "my_pdfs = make_function_objects(pdfs)\n",
    "my_cdfs = [pdf.integrate() for pdf in my_pdfs]\n",
    "my_qfs = [cdf.invert() for cdf in my_cdfs]\n",
    "my_qdfs = [qf.differentiate() for qf in my_qfs]\n",
    "# For numerical correction: shorten the range for smaller sigmas to get rid of\n",
    "# numerical artifacts when computing integrals, derivatives and means later\n",
    "new_ranges = get_optimal_range(my_pdfs)\n",
    "# Generate pdfs again, this time within individual ranges\n",
    "pdfs2 = [\n",
    "    make_truncnorm_pdf(\n",
    "        new_ranges[i][0],\n",
    "        new_ranges[i][1],\n",
    "        mus[i],\n",
    "        sigmas[i],\n",
    "        grid_size=grid_size,\n",
    "        warn_irregular_densities=False,\n",
    "    )[0]\n",
    "    for i in range(n)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate all the distribution objects\n",
    "new_pdfs = make_function_objects(pdfs2)\n",
    "new_cdfs = [pdf.integrate() for pdf in new_pdfs]\n",
    "new_qfs = [cdf.invert() for cdf in new_cdfs]\n",
    "new_qdfs = [qf.differentiate() for qf in new_qfs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute centered data, just to see whether it works\n",
    "# One can clearly see how inappropriate it is to apply fda methods on densities\n",
    "mean_pdf, centered_pdfs = compute_centered_data(new_pdfs)\n",
    "centered_pdfs[sigmas.argmin()].compare(new_pdfs[sigmas.argmin()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covariance_function = compute_cov_function(centered_pdfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalues, eigenfunctions = compute_principal_components(\n",
    "    centered_pdfs[0].x,\n",
    "    covariance_function,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation FPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform pdf sample, and test whether inverse works\n",
    "log_qdfs = log_qd_transform(new_pdfs)\n",
    "inverse_log_qdfs = inverse_log_qd_transform(log_qdfs)\n",
    "inverse_log_qdfs[0].compare(new_pdfs[0] + 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute transformation FPCA objects\n",
    "pcs_to_compute = 10\n",
    "(\n",
    "    mean_log_qdfs,\n",
    "    eigenvalues_log_qdfs,\n",
    "    eigenfunctions_log_qdfs,\n",
    "    fpc_scores_log_qdfs,\n",
    ") = gen_qdtransformation_pcs(log_qdfs, k=pcs_to_compute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Karhunen-Loève decomposition of transforms\n",
    "truncation_k = 1\n",
    "truncated_representations_transforms = karhunen_loeve(\n",
    "    mean_log_qdfs,\n",
    "    eigenfunctions_log_qdfs,\n",
    "    fpc_scores_log_qdfs,\n",
    "    K=truncation_k,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform to density space. Compare truncated representation against real density\n",
    "truncated_representations = inverse_log_qd_transform(\n",
    "    truncated_representations_transforms,\n",
    ")\n",
    "truncated_representations[150].warp_range(-trunc, trunc).compare(\n",
    "    new_pdfs[150].warp_range(-trunc, trunc),\n",
    "    label_self=f\"Truncated at K = {truncation_k}\",\n",
    "    label_other=\"Original Density\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at modes of variance of transformed functions\n",
    "variation_modes_transforms = [\n",
    "    mode_of_variation(mean_log_qdfs, eigval, eigfunc, alpha=5e-3)\n",
    "    for eigval, eigfunc\n",
    "    in zip(eigenvalues_log_qdfs, eigenfunctions_log_qdfs, strict=True)\n",
    "]\n",
    "variation_modes_transforms[0].compare(variation_modes_transforms[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate modes of variation to density space, compare first two modes\n",
    "variation_modes = inverse_log_qd_transform(variation_modes_transforms)\n",
    "variation_modes[0].compare(\n",
    "    variation_modes[1],\n",
    "    label_self=\"1st mode\",\n",
    "    label_other=\"2nd mode\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Fréchet mean\n",
    "f_mean = frechet_mean(new_pdfs)\n",
    "f_mean.warp_range(-trunc, trunc).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Fréchet variance\n",
    "total_variance = total_frechet_variance(f_mean, new_pdfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try function that finds optimal trunc representation\n",
    "optimal_k, fraction_explained, truncated_representations = k_optimal(\n",
    "    0.5,\n",
    "    total_variance,\n",
    "    new_pdfs,\n",
    "    mean_log_qdfs,\n",
    "    eigenfunctions_log_qdfs,\n",
    "    fpc_scores_log_qdfs,\n",
    ")\n",
    "optimal_k, fraction_explained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of simulations\n",
    "m = 5\n",
    "sample_sizes = [50, 100, 200]\n",
    "# Initialize arrays to store Fréchet and cross sectional means\n",
    "stored_f_means = np.empty((m, len(sample_sizes)), dtype=\"object\")\n",
    "stored_cs_means = np.empty((m, len(sample_sizes)), dtype=\"object\")\n",
    "stored_f_means_denstimation = np.empty((m, len(sample_sizes)), dtype=\"object\")\n",
    "stored_cs_means_denstimation = np.empty((m, len(sample_sizes)), dtype=\"object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Simulating...\", end=\"\\r\")\n",
    "for i in range(m):\n",
    "    # Try different sample sizes\n",
    "    for j, n in enumerate(sample_sizes):\n",
    "        # j index only used for storing Fréchet means below!\n",
    "        # Set parameters\n",
    "        grid_size = 10000\n",
    "        trunc = 3\n",
    "        seed_num = int(str(i) + str(n))  # unique seed in each simulation run\n",
    "        mus, sigmas = gen_params_scenario_one(n, seed=seed_num)\n",
    "        # Sort sigmas, because when summing Function instances something goes wrong otherwise\n",
    "        sigmas.sort()\n",
    "\n",
    "        # Generate pdfs within truncation points\n",
    "        pdfs = make_truncnorm_pdf(\n",
    "            -trunc,\n",
    "            trunc,\n",
    "            mus,\n",
    "            sigmas,\n",
    "            grid_size=grid_size,\n",
    "            warn_irregular_densities=False,\n",
    "        )\n",
    "        # Make Function class objects\n",
    "        my_pdfs = make_function_objects(pdfs)\n",
    "\n",
    "        # For numerical correction: shorten the range for smaller sigmas to get rid of\n",
    "        # numerical artifacts when computing integrals, derivatives and means later\n",
    "        new_ranges = get_optimal_range(my_pdfs)\n",
    "        # Generate pdfs again, this time within individual ranges\n",
    "        pdfs2 = [\n",
    "            make_truncnorm_pdf(\n",
    "                new_ranges[i][0],\n",
    "                new_ranges[i][1],\n",
    "                mus[i],\n",
    "                sigmas[i],\n",
    "                grid_size=grid_size,\n",
    "                warn_irregular_densities=False,\n",
    "            )[0]\n",
    "            for i in range(n)\n",
    "        ]\n",
    "\n",
    "        # Generate numerically stable objects\n",
    "        new_pdfs = make_function_objects(pdfs2)\n",
    "\n",
    "        # Compute Fréchet mean\n",
    "        stored_f_means[i, j] = frechet_mean(new_pdfs)\n",
    "        stored_cs_means[i, j] = mean_func(new_pdfs)\n",
    "\n",
    "        # Compute Fréchet variance\n",
    "        total_variance = total_frechet_variance(stored_f_means[i, j], new_pdfs)\n",
    "\n",
    "        # Transform pdf sample\n",
    "        log_qdfs = log_qd_transform(new_pdfs)\n",
    "\n",
    "        (\n",
    "            mean_log_qdfs,\n",
    "            eigenvalues_log_qdfs,\n",
    "            eigenfunctions_log_qdfs,\n",
    "            fpc_scores_log_qdfs,\n",
    "        ) = gen_qdtransformation_pcs(log_qdfs, k=2)\n",
    "\n",
    "        # # Try function that finds optimal trunc representation\n",
    "        optimal_k, fraction_explained, truncated_representations = k_optimal(\n",
    "            0.5,\n",
    "            total_variance,\n",
    "            new_pdfs,\n",
    "            mean_log_qdfs,\n",
    "            eigenfunctions_log_qdfs,\n",
    "            fpc_scores_log_qdfs,\n",
    "        )\n",
    "\n",
    "        ### And now again with additional density estimation step!\n",
    "        which_kernel = \"std_normal\"\n",
    "        sample_points = gen_truncnorm_pdf_points(-trunc, trunc, mus, sigmas, 100)\n",
    "        pdf_hats = make_estimated_truncnorm_pdf(\n",
    "            sample_points=sample_points,\n",
    "            a=-trunc * np.ones(n),\n",
    "            b=trunc * np.ones(n),\n",
    "            kern=which_kernel,\n",
    "            grid_size=grid_size,\n",
    "            bandwidth=0.2,\n",
    "        )\n",
    "\n",
    "        new_ranges = get_optimal_range(pdf_hats)\n",
    "        # Generate numerically stable objects, within individual ranges\n",
    "        new_pdf_hats = make_estimated_truncnorm_pdf(\n",
    "            sample_points=sample_points,\n",
    "            a=new_ranges[:, 0],\n",
    "            b=new_ranges[:, 1],\n",
    "            kern=which_kernel,\n",
    "            grid_size=grid_size,\n",
    "            bandwidth=0.2,\n",
    "        )\n",
    "\n",
    "        # Compute Fréchet mean\n",
    "        stored_f_means_denstimation[i, j] = frechet_mean(new_pdf_hats)\n",
    "        stored_cs_means_denstimation[i, j] = mean_func(new_pdf_hats)\n",
    "\n",
    "        # # Compute Fréchet variance\n",
    "        total_variance = total_frechet_variance(\n",
    "            stored_f_means_denstimation[i, j], new_pdf_hats,\n",
    "        )\n",
    "\n",
    "        # # Transform pdf sample\n",
    "        log_qdfs = log_qd_transform(new_pdf_hats)\n",
    "\n",
    "        (\n",
    "            mean_log_qdfs,\n",
    "            eigenvalues_log_qdfs,\n",
    "            eigenfunctions_log_qdfs,\n",
    "            fpc_scores_log_qdfs,\n",
    "        ) = gen_qdtransformation_pcs(log_qdfs, k=2)\n",
    "\n",
    "        # Try function that finds optimal trunc representation. Threshold: explain\n",
    "        # at least 80 %\n",
    "        optimal_k, fraction_explained, truncated_representations = k_optimal(\n",
    "            0.8,\n",
    "            total_variance,\n",
    "            new_pdf_hats,\n",
    "            mean_log_qdfs,\n",
    "            eigenfunctions_log_qdfs,\n",
    "            fpc_scores_log_qdfs,\n",
    "        )\n",
    "    perc = int(100 * (i + 1) / m)\n",
    "    print(f\"Simulating...{perc}%\", end=\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate true center of distribution for plotting against estimates\n",
    "std_normal = make_truncnorm_pdf(-trunc, trunc, 0, 1, grid_size=grid_size)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Fréchet means of Fréchet means over all simulations\n",
    "mean_of_f_means50 = frechet_mean(stored_f_means[:, 0])\n",
    "mean_of_f_means100 = frechet_mean(stored_f_means[:, 1])\n",
    "mean_of_f_means200 = frechet_mean(stored_f_means[:, 2])\n",
    "mean_of_cs_means50 = frechet_mean(stored_cs_means[:, 0])\n",
    "mean_of_cs_means100 = frechet_mean(stored_cs_means[:, 1])\n",
    "mean_of_cs_means200 = frechet_mean(stored_cs_means[:, 2])\n",
    "mean_of_f_means50_denstimation = frechet_mean(stored_f_means_denstimation[:, 0])\n",
    "mean_of_f_means100_denstimation = frechet_mean(stored_f_means_denstimation[:, 1])\n",
    "mean_of_f_means200_denstimation = frechet_mean(stored_f_means_denstimation[:, 2])\n",
    "mean_of_cs_means50_denstimation = frechet_mean(stored_cs_means_denstimation[:, 0])\n",
    "mean_of_cs_means100_denstimation = frechet_mean(stored_cs_means_denstimation[:, 1])\n",
    "mean_of_cs_means200_denstimation = frechet_mean(stored_cs_means_denstimation[:, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Fréchet means of different sample sizes against true center\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(mean_of_f_means50.x, mean_of_f_means50.y, label=\"Size 50\", linestyle=\"--\")\n",
    "ax.plot(mean_of_f_means100.x, mean_of_f_means100.y, label=\"Size 100\", linestyle=\"--\")\n",
    "ax.plot(mean_of_f_means200.x, mean_of_f_means200.y, label=\"Size 200\", linestyle=\"--\")\n",
    "ax.plot(std_normal[0], std_normal[1], label=\"Standard Normal\", color=\"Black\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Fréchet mean vs cross-sectional mean and true center\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(\n",
    "    mean_of_f_means200.x, mean_of_f_means200.y, label=\"Fréchet Mean\", linestyle=\"--\",\n",
    ")\n",
    "ax.plot(\n",
    "    mean_of_cs_means200.x,\n",
    "    mean_of_cs_means200.y,\n",
    "    label=\"Cross-Sectional Mean\",\n",
    "    linestyle=\"--\",\n",
    ")\n",
    "ax.plot(std_normal[0], std_normal[1], label=\"Standard Normal\", color=\"black\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Fréchet means from estimated densities and of different sample sizes against true center\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(\n",
    "    mean_of_f_means50_denstimation.x,\n",
    "    mean_of_f_means50_denstimation.y,\n",
    "    label=\"Size 50\",\n",
    "    linestyle=\"--\",\n",
    ")\n",
    "ax.plot(\n",
    "    mean_of_f_means100_denstimation.x,\n",
    "    mean_of_f_means100_denstimation.y,\n",
    "    label=\"Size 100\",\n",
    "    linestyle=\"--\",\n",
    ")\n",
    "ax.plot(\n",
    "    mean_of_f_means200_denstimation.x,\n",
    "    mean_of_f_means200_denstimation.y,\n",
    "    label=\"Size 200\",\n",
    "    linestyle=\"--\",\n",
    ")\n",
    "ax.plot(std_normal[0], std_normal[1], label=\"Standard Normal\", color=\"Black\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Fréchet mean with true densities against estimated densities and true center\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(\n",
    "    mean_of_f_means200.x,\n",
    "    mean_of_f_means200.y,\n",
    "    label=\"True Densities\",\n",
    "    linestyle=\"--\",\n",
    ")\n",
    "ax.plot(\n",
    "    mean_of_f_means200_denstimation.x,\n",
    "    mean_of_f_means200_denstimation.y,\n",
    "    label=\"Estimated Densities\",\n",
    "    linestyle=\"--\",\n",
    ")\n",
    "ax.plot(std_normal[0], std_normal[1], label=\"Standard Normal\", color=\"black\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Fréchet mean with true densities against estimated densities and true center\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(\n",
    "    mean_of_f_means200.x,\n",
    "    mean_of_f_means200.y,\n",
    "    label=\"True Densities\",\n",
    "    linestyle=\"--\",\n",
    ")\n",
    "ax.plot(\n",
    "    mean_of_f_means200_denstimation.x,\n",
    "    mean_of_f_means200_denstimation.y,\n",
    "    label=\"Estimated Densities\",\n",
    "    linestyle=\"--\",\n",
    ")\n",
    "ax.plot(std_normal[0], std_normal[1], label=\"Standard Normal\", color=\"black\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement density estimation step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 200\n",
    "grid_size = 1000\n",
    "trunc = 3\n",
    "mus, sigmas = gen_params_scenario_one(n, seed=123456)\n",
    "# Sort sigmas, because when summing Function instances something goes wrong otherwise\n",
    "sigmas.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_points = gen_truncnorm_pdf_points(-trunc, trunc, mus, sigmas, 100)\n",
    "pdf_hats = make_estimated_truncnorm_pdf(\n",
    "    sample_points=sample_points,\n",
    "    a=-trunc * np.ones(n),\n",
    "    b=trunc * np.ones(n),\n",
    "    kern=\"std_normal\",\n",
    "    grid_size=grid_size,\n",
    "    bandwidth=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_hats[0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = make_kernel_functions(\n",
    "    np.linspace((-trunc) * np.ones(n), trunc * np.ones(n), 10000).transpose(),\n",
    "    h=0.2,\n",
    "    kernel=\"std_normal\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from function_class import Function\n",
    "\n",
    "kernels = {\n",
    "    \"epanechnikov\": lambda u: 3\n",
    "    / (4 * np.sqrt(5))\n",
    "    * (1 - (u**2) / 5)\n",
    "    * (np.abs(u) <= np.sqrt(5)),\n",
    "    \"uniform\": lambda u: 0.5 * (np.abs(u) <= 1),\n",
    "    \"triangular\": lambda u: (1 - np.abs(u)) * (np.abs(u) <= 1),\n",
    "    \"std_normal\": lambda u: (1 / np.sqrt(2 * np.pi))\n",
    "    * np.exp(-0.5 * u**2),  # p. 8 Li/Racine\n",
    "}\n",
    "\n",
    "\n",
    "def make_kernel_functions(x_vals: np.ndarray, kernel: str = \"epanechnikov\"):\n",
    "    \"\"\"Return kernel function as Function class object.\"\"\"\n",
    "    k = kernels[kernel]\n",
    "    return [\n",
    "        Function(kernel_x, kernel_y)\n",
    "        for kernel_x, kernel_y in zip(x_vals, k(x_vals), strict=True)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def density_estimator_better(\n",
    "    x: np.ndarray,\n",
    "    sample_of_points: np.ndarray,\n",
    "    h: float,\n",
    "    kernel_type: str = \"epanechnikov\",\n",
    "):\n",
    "    \"\"\"Calculate boundary corrected density estimator from Petersen & Müller 2016.\"\"\"\n",
    "    def standardize(r):\n",
    "        return r - x[:, 0][:, np.newaxis] / (x[:, -1][:, np.newaxis] - x[:, 0][:, np.newaxis])\n",
    "    x_std = standardize(x)\n",
    "    sample_std = standardize(sample_of_points)\n",
    "    k = kernels[kernel_type]\n",
    "\n",
    "    result = np.zeros_like(x_std)\n",
    "\n",
    "    if sample_of_points.ndim > 1:\n",
    "        for i, density in enumerate(sample_of_points):  # Looping over densities\n",
    "            for point in density:  # Looping over samples for each density\n",
    "                u = (pdfs_x[i] - point) / h\n",
    "                result[i, :] += k(u)\n",
    "        pdfs_y = result / (len(density) * h)\n",
    "        list_of_densities = [\n",
    "            (pdf_x, pdf_y) for pdf_x, pdf_y in zip(pdfs_x, pdfs_y, strict=True)\n",
    "        ]\n",
    "    else:\n",
    "        # Add axes to make use of broadcasting rules and vectorization\n",
    "        u = (pdfs_x[:, np.newaxis] - sample_of_points[np.newaxis, :]) / h\n",
    "        result += np.sum(k(u), axis=1)\n",
    "        pdfs_y = result / (len(sample_of_points) * h)\n",
    "        list_of_densities = [\n",
    "            (pdf_x, pdf_y) for pdf_x, pdf_y in zip(pdfs_x, pdfs_y, strict=True)\n",
    "        ]\n",
    "    return np.einsum(\"ij,ik->ijk\", x_std, sample_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = density_estimator_better(\n",
    "    np.linspace(-2 * np.ones(n), 2 * np.ones(n), 10000).transpose(), sample_points, 0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the weight function w(x, h) as described in the paper Petersen & Müller 2016\n",
    "def weight_function(x: np.ndarray, h: float, kernel_type: str = \"epanechnikov\"):\n",
    "    np.zeros_like(x)\n",
    "    return make_kernel_functions(x_vals=x / h, kernel=kernel_type)\n",
    "\n",
    "    # # Case for x in [0, h)\n",
    "    # # weight[mask1] =\n",
    "\n",
    "    # # Case for x in (1-h, 1]\n",
    "\n",
    "    # # Case for x in [h, 1-h]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = weight_function(\n",
    "    np.linspace((-trunc) * np.ones(n), trunc * np.ones(n), 10000).transpose(),\n",
    "    h=0.2,\n",
    "    kernel_type=\"std_normal\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp[0].x[x >= 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def density_estimator(\n",
    "    x_vals: np.ndarray,\n",
    "    sample_of_points: np.ndarray,\n",
    "    h,\n",
    "    kernel_type=\"std_normal\",\n",
    "):\n",
    "    \"\"\"Kernel density estimator function. Assumes each row in `sample_of_points` is a\n",
    "    density, and the columns represent the number of realizations. Each row in\n",
    "    `x_vals` corresponds to the grid, on which density is to be estimated.\n",
    "    \"\"\"\n",
    "    # Select kernel function\n",
    "    k = kernels[kernel_type]\n",
    "    # To make possibly scalar x_vals compatible with array operations\n",
    "    x_vals = np.atleast_1d(x_vals)\n",
    "    # Pre-allocate the result array for more speed\n",
    "    result = np.zeros_like(x_vals)\n",
    "\n",
    "    if sample_of_points.ndim > 1:\n",
    "        n_densities, n_samples = sample_of_points.shape\n",
    "        for i in range(n_densities):  # Looping over densities\n",
    "            for j in range(n_samples):  # Looping over samples for each density\n",
    "                u = (x_vals[i] - sample_of_points[i, j]) / h\n",
    "                result[i, :] += k(u)\n",
    "    else:\n",
    "        n_samples = len(sample_of_points)\n",
    "        # Add axes to make use of broadcasting rules and vectorization\n",
    "        u = (x_vals[:, np.newaxis] - sample_of_points[np.newaxis, :]) / h\n",
    "        result += np.sum(k(u), axis=1)\n",
    "    return result / (n_samples * h)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
