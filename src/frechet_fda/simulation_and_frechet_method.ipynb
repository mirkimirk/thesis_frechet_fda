{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This module implements the Fréchet regression method from Petersen and Müller 2019.\n",
    "\n",
    "The method is implemented in the context of estimation in the space of distribution\n",
    "functions. Also, a simulation study is designed to compare with the methods outlined in\n",
    "Petersen and Müller 2016.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import quadprog\n",
    "from scipy.stats import norm\n",
    "\n",
    "from frechet_fda.function_class import Function\n",
    "from frechet_fda.tools.function_tools import mean_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 200\n",
    "seed = 28071995\n",
    "predictor = np.random.default_rng(seed).uniform(-1, 1, N)\n",
    "predictor.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mu_from_x(x, mu0, beta, v1):\n",
    "    \"\"\"Sample mus from the first simulation setting in Petersen & Müller (2019).\"\"\"\n",
    "    return np.random.default_rng(seed).normal(loc=mu0 + beta * x, scale=v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mus = mu_from_x(predictor, 0, 3, 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma_from_x(x, sigma0, gamma, v2):\n",
    "    \"\"\"Sample sigmas from the first simulation setting Petersen & Müller (2019).\"\"\"\n",
    "    sh = (sigma0 + gamma * x) ** 2 / v2\n",
    "    sc = v2 / (sigma0 + gamma * x)\n",
    "\n",
    "    return np.random.default_rng(seed).gamma(shape=sh, scale=sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmas = sigma_from_x(predictor, 3, 0.5, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size = 500\n",
    "delta = 0\n",
    "u = np.linspace(delta, 1 - delta, grid_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_y_qf(mu, sigma, eval_points):\n",
    "    \"\"\"Generate quantile function of Y_i given X_i.\"\"\"\n",
    "    ys = (\n",
    "        mu[..., np.newaxis]\n",
    "        + sigma[..., np.newaxis] * norm.ppf(eval_points)[np.newaxis, ...]\n",
    "    )\n",
    "    return [Function(eval_points, y) for y in ys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qfs = gen_y_qf(mus, sigmas, u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qfs[0].y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qfs[0].compare(qfs[199], label_self=\"x = -1\", label_other=\"x = 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(\n",
    "    qfs[0].x,\n",
    "    qfs[0].y,\n",
    "    label=f\"Mu = {round(mus[0], 2)}, Sigma = {round(sigmas[0], 2)}\",\n",
    ")\n",
    "ax.plot(\n",
    "    qfs[199].x,\n",
    "    qfs[199].y,\n",
    "    label=f\"Mu = {round(mus[199], 2)}, Sigma = {round(sigmas[199], 2)}\",\n",
    ")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "for qf in qfs[:6]:\n",
    "    pdf = qf.invert().differentiate()\n",
    "    ax.plot(\n",
    "        pdf.x,\n",
    "        pdf.y,\n",
    "    )\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _empirical_weight_function(x_eval, predictor_values):\n",
    "    \"\"\"Weight function to compute weighted Fréchet mean.\"\"\"\n",
    "    predictor_values = np.atleast_2d(predictor_values)\n",
    "    means = np.mean(predictor_values, axis=-1)\n",
    "    cov_matrix = (\n",
    "        predictor_values @ predictor_values.transpose() / predictor_values.shape[-1]\n",
    "    )\n",
    "    inv_cov_matrix = np.linalg.inv(cov_matrix)\n",
    "    return 1 + (\n",
    "        (predictor_values - means[..., np.newaxis]).transpose()\n",
    "        @ inv_cov_matrix\n",
    "        @ (x_eval - means)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qf_tilde(quantile_funcs, x, predictor_values):\n",
    "    \"\"\"Estimator for quantile function.\"\"\"\n",
    "    return mean_func(_empirical_weight_function(x, predictor_values) * quantile_funcs).y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take only very tenth value of predictor grid to calculate estimate\n",
    "x_grid = predictor[::10]\n",
    "x_grid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_qp(u, x_grid, x_observed, quantile_functions) -> list[Function]:\n",
    "    \"\"\"Sets up quadratic programming problem.\n",
    "\n",
    "    quantile_functions need to have a support of len(u).\n",
    "    \"\"\"\n",
    "    grid_size = len(u)\n",
    "    qp_g = np.identity(grid_size)  # make sure P is symmetric\n",
    "    qp_c = np.eye(grid_size, grid_size - 1, k=-1) - np.eye(grid_size, grid_size - 1)\n",
    "    qp_b = np.zeros(grid_size - 1)\n",
    "    estimates = []\n",
    "    for x in x_grid:\n",
    "        qp_a = qf_tilde(quantile_functions, x, x_observed)\n",
    "        constraints_check = qp_a[1:] - qp_a[:-1]\n",
    "        # remove nans\n",
    "        constraints_check = constraints_check[~np.isnan(constraints_check)]\n",
    "        if np.all(constraints_check > 0):\n",
    "            # If estimator valid qf, it is the optimal solution\n",
    "            solution = qp_a\n",
    "        else:\n",
    "            # Else, find closest vector to estimator that is valid solution\n",
    "            solution = quadprog.solve_qp(qp_g, qp_a, qp_c, qp_b)[0]\n",
    "        estimates.append(Function(u, solution))\n",
    "    return estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimates = solve_qp(u, x_grid, predictor, qfs)\n",
    "# For better visualization in 3d plot\n",
    "estimates = [estimate.drop_inf() for estimate in estimates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for func in estimates[:5]:\n",
    "    plt.plot(func.x, func.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a 2D array to hold f(u, x) values\n",
    "z_values = np.zeros((len(x_grid), len(func.x)))\n",
    "\n",
    "# Populate the array with estimates\n",
    "for i, func in enumerate(estimates):\n",
    "    z_values[i] = func.y\n",
    "\n",
    "# Create the 3D surface plot\n",
    "X, Y = np.meshgrid(func.x, x_grid)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "ax.plot_surface(X, Y, z_values, cmap=\"viridis\")\n",
    "\n",
    "# Adjust the viewing angle for better visibility\n",
    "ax.view_init(elev=15, azim=255)\n",
    "\n",
    "ax.set_xlabel(\"u\")\n",
    "ax.set_ylabel(\"Predictor x\")\n",
    "ax.set_zlabel(\"f(u, x)\", rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Integrated Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from frechet_fda.tools.function_tools import quantile_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_hat = [estimate.drop_inf() for estimate in estimates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_m = qfs[::10]\n",
    "true_m = [qf.drop_inf() for qf in true_m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "differences = [hat - true for hat, true in zip(m_hat, true_m, strict=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = [\n",
    "    quantile_distance(hat, true, already_qf=True)\n",
    "    for hat, true in zip(m_hat, true_m, strict=True)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ise_wasserstein(m_hat, true_m, x_grid):\n",
    "    \"\"\"Compute integrated squared error.\"\"\"\n",
    "    distances = [\n",
    "        quantile_distance(hat, true, already_qf=True)\n",
    "        for hat, true in zip(m_hat, true_m, strict=True)\n",
    "    ]\n",
    "    return Function(x_grid, distances).integrate().y[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ise_wasserstein(m_hat, true_m, x_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functional Regression with Transformation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from frechet_fda.tools.function_tools import (\n",
    "    inverse_log_qd_transform,\n",
    "    log_qd_transform,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mus[:5], sigmas[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see whether toying around with function and returning to it changes anything\n",
    "qfs[199].drop_inf().differentiate().integrate().compare(qfs[199].drop_inf() + 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfs = [qf.drop_inf().invert().differentiate() for qf in qfs]\n",
    "pdfs_normalized = [pdf.standardize_support() for pdf in pdfs]\n",
    "lqdfs_and_start_vals = log_qd_transform(pdfs, different_supports=True)\n",
    "lqdfs = lqdfs_and_start_vals[:, 0]\n",
    "start_vals = lqdfs_and_start_vals[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_matrix = np.array((np.ones(len(predictor)), predictor)).transpose()\n",
    "predictor_matrix.shape, np.linalg.inv(\n",
    "    predictor_matrix.transpose() @ predictor_matrix,\n",
    ").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_betahat = (\n",
    "    np.linalg.inv(predictor_matrix.transpose() @ predictor_matrix)\n",
    "    @ predictor_matrix.transpose()\n",
    "    @ lqdfs\n",
    ")\n",
    "log_betahat0 = log_betahat[0]\n",
    "log_betahat1 = log_betahat[1]\n",
    "log_betahat0.compare(log_betahat1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lqdf_hat = predictor * log_betahat1 + log_betahat0\n",
    "lqdfs[0].compare(lqdf_hat[0])\n",
    "lqdfs[199].compare(lqdf_hat[199])\n",
    "pdf_hat = inverse_log_qd_transform(lqdf_hat, start_vals)\n",
    "pdfs[0].compare(pdf_hat[0])\n",
    "pdfs[199].compare(pdf_hat[199])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mus[0], mus[99], mus[199], sigmas[0], sigmas[99], sigmas[199]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution_of_interest = 180\n",
    "pdfs[distribution_of_interest].compare(pdf_hat[distribution_of_interest])\n",
    "qfs[distribution_of_interest].compare(\n",
    "    pdf_hat[distribution_of_interest].integrate().invert(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of simulations\n",
    "m = 5\n",
    "# Number of samples of densities to generate\n",
    "sample_sizes = [200]\n",
    "# Number of sample points to generate from each density for density estimation step\n",
    "points_number = 100\n",
    "# Bandwidth choice for density estimation\n",
    "bandwidth_choice = 0.2\n",
    "# Fineness of grids to evaluate functions\n",
    "grid_size = 1000\n",
    "# Where to truncate normal distribution\n",
    "trunc = 3\n",
    "#\n",
    "threshold_variance_explained = 0.9\n",
    "# Initialize arrays to store Fréchet and cross sectional means\n",
    "stored_f_means = np.empty((m, len(sample_sizes)), dtype=\"object\")\n",
    "stored_cs_means = np.empty((m, len(sample_sizes)), dtype=\"object\")\n",
    "stored_f_means_denstimation = np.empty((m, len(sample_sizes)), dtype=\"object\")\n",
    "stored_cs_means_denstimation = np.empty((m, len(sample_sizes)), dtype=\"object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_iterations = m * len(sample_sizes)  # Total iterations\n",
    "current_iteration = 0  # Initialize a counter for the current iteration\n",
    "print(\"Simulating...\", end=\"\\r\")\n",
    "for i in range(m):\n",
    "    # Try different sample sizes\n",
    "    for j, n in enumerate(sample_sizes):\n",
    "        # j index only used for storing Fréchet means below!\n",
    "        # Set parameters\n",
    "        seed_num = int(str(i) + str(j))  # unique seed in each simulation run\n",
    "        mus, sigmas = gen_params_scenario_one(n, seed=seed_num)\n",
    "        # Sort sigmas just for inspection reasons\n",
    "        sigmas.sort()\n",
    "\n",
    "        # Generate pdfs within truncation points\n",
    "        pdfs = make_truncnorm_pdf(\n",
    "            -trunc,\n",
    "            trunc,\n",
    "            mus,\n",
    "            sigmas,\n",
    "            grid_size=grid_size,\n",
    "            warn_irregular_densities=False,\n",
    "        )\n",
    "        # Make Function class objects\n",
    "        my_pdfs = make_function_objects(pdfs)\n",
    "\n",
    "        # For numerical correction: shorten the range for smaller sigmas to get rid of\n",
    "        # numerical artifacts when computing integrals, derivatives and means later\n",
    "        new_ranges = get_optimal_range(my_pdfs)\n",
    "        # Generate pdfs again, this time within individual ranges\n",
    "        pdfs2 = [\n",
    "            make_truncnorm_pdf(\n",
    "                new_ranges[i][0],\n",
    "                new_ranges[i][1],\n",
    "                mus[i],\n",
    "                sigmas[i],\n",
    "                grid_size=grid_size,\n",
    "                warn_irregular_densities=False,\n",
    "            )[0]\n",
    "            for i in range(n)\n",
    "        ]\n",
    "\n",
    "        # Generate numerically stable objects\n",
    "        new_pdfs = make_function_objects(pdfs2)\n",
    "\n",
    "        # Compute Fréchet mean\n",
    "        stored_f_means[i, j] = frechet_mean(new_pdfs)\n",
    "        stored_cs_means[i, j] = mean_func(new_pdfs)\n",
    "\n",
    "        # Compute Fréchet variance\n",
    "        total_variance = total_frechet_variance(stored_f_means[i, j], new_pdfs)\n",
    "\n",
    "        # Transform pdf sample\n",
    "        log_qdfs = log_qd_transform(new_pdfs)\n",
    "        (\n",
    "            mean_log_qdfs,\n",
    "            eigenvalues_log_qdfs,\n",
    "            eigenfunctions_log_qdfs,\n",
    "            fpc_scores_log_qdfs,\n",
    "        ) = gen_qdtransformation_pcs(log_qdfs, k=2)\n",
    "\n",
    "        # Try function that finds optimal trunc representation. Threshold: explain\n",
    "        # at least threshold_variance_explained\n",
    "        optimal_k, fraction_explained, truncated_representations = k_optimal(\n",
    "            threshold_variance_explained,\n",
    "            total_variance,\n",
    "            new_pdfs,\n",
    "            mean_log_qdfs,\n",
    "            eigenfunctions_log_qdfs,\n",
    "            fpc_scores_log_qdfs,\n",
    "        )\n",
    "\n",
    "        ### And now again with additional density estimation step!\n",
    "        which_kernel = \"std_normal\"\n",
    "        sample_points = gen_truncnorm_pdf_points(\n",
    "            -trunc,\n",
    "            trunc,\n",
    "            mus,\n",
    "            sigmas,\n",
    "            points_number,\n",
    "        )\n",
    "        pdf_hats = make_estimated_truncnorm_pdf(\n",
    "            sample_points=sample_points,\n",
    "            a=-trunc * np.ones(n),\n",
    "            b=trunc * np.ones(n),\n",
    "            kern=which_kernel,\n",
    "            grid_size=grid_size,\n",
    "            bandwidth=bandwidth_choice,\n",
    "        )\n",
    "\n",
    "        new_ranges = get_optimal_range(pdf_hats)\n",
    "        # Generate numerically stable objects, within individual ranges\n",
    "        new_pdf_hats = make_estimated_truncnorm_pdf(\n",
    "            sample_points=sample_points,\n",
    "            a=new_ranges[:, 0],\n",
    "            b=new_ranges[:, 1],\n",
    "            kern=which_kernel,\n",
    "            grid_size=grid_size,\n",
    "            bandwidth=bandwidth_choice,\n",
    "        )\n",
    "\n",
    "        # Compute Fréchet mean\n",
    "        stored_f_means_denstimation[i, j] = frechet_mean(new_pdf_hats)\n",
    "        stored_cs_means_denstimation[i, j] = mean_func(new_pdf_hats)\n",
    "\n",
    "        # Compute Fréchet variance\n",
    "        total_variance = total_frechet_variance(\n",
    "            stored_f_means_denstimation[i, j],\n",
    "            new_pdf_hats,\n",
    "        )\n",
    "\n",
    "        # Transform pdf sample\n",
    "        log_qdfs = log_qd_transform(new_pdf_hats)\n",
    "\n",
    "        (\n",
    "            mean_log_qdfs,\n",
    "            eigenvalues_log_qdfs,\n",
    "            eigenfunctions_log_qdfs,\n",
    "            fpc_scores_log_qdfs,\n",
    "        ) = gen_qdtransformation_pcs(log_qdfs, k=2)\n",
    "\n",
    "        # Try function that finds optimal trunc representation. Threshold: explain\n",
    "        # at least threshold_variance_explained\n",
    "        optimal_k, fraction_explained, truncated_representations = k_optimal(\n",
    "            threshold_variance_explained,\n",
    "            total_variance,\n",
    "            new_pdf_hats,\n",
    "            mean_log_qdfs,\n",
    "            eigenfunctions_log_qdfs,\n",
    "            fpc_scores_log_qdfs,\n",
    "        )\n",
    "\n",
    "        # Print progress of simulation\n",
    "        current_iteration += 1  # Increment the current iteration\n",
    "        perc = int(100 * current_iteration / total_iterations)\n",
    "        print(f\"Simulating...{perc}%\", end=\"\\r\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
