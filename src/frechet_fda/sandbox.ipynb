{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Sandbox module.\"\"\"\n",
    "from functools import partial\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import skfda\n",
    "from scipy import stats\n",
    "from skfda.preprocessing.dim_reduction import FPCA\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nonparametric Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel density estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels = {\n",
    "    \"Epanechnikov\": lambda u: 3\n",
    "    / (4 * np.sqrt(5))\n",
    "    * (1 - (u**2) / 5)\n",
    "    * int(abs(u) <= np.sqrt(5)),\n",
    "    \"Uniform\": lambda u: 0.5 * int(abs(u) <= 1),\n",
    "    \"Triangular\": lambda u: (1 - abs(u)) * int(abs(u) <= 1),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_estimator(x, h, sample, kernel_type):\n",
    "    \"\"\"Kernel density estimator function.\"\"\"\n",
    "    k = np.vectorize(kernels[kernel_type])\n",
    "    return 1 / (len(sample) * h) * np.sum(k((x - sample) / h))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix parameters and generate sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 200\n",
    "n_grid = 100\n",
    "grid_ending = 10\n",
    "mu = 0\n",
    "sigma = 10\n",
    "\n",
    "sample = np.random.default_rng(seed=28071995).normal(loc=mu, scale=sigma, size=n)\n",
    "grid = np.linspace(start=-grid_ending, stop=grid_ending, num=n_grid)\n",
    "# Rule-of-Thumb bandwidth (Li and Racine 2007, p. 66)\n",
    "bandwidth = np.std(sample) * (n ** (-0.2))  # should implement optimal bandwidth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_estimator_given_sample = partial(kernel_estimator, sample=sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate fitted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_epa = [\n",
    "    kernel_estimator_given_sample(x=i, h=bandwidth, kernel_type=\"Epanechnikov\")\n",
    "    for i in grid\n",
    "]\n",
    "values_uni = [\n",
    "    kernel_estimator_given_sample(x=i, h=bandwidth, kernel_type=\"Uniform\") for i in grid\n",
    "]\n",
    "values_tri = [\n",
    "    kernel_estimator_given_sample(x=i, h=bandwidth, kernel_type=\"Triangular\")\n",
    "    for i in grid\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(grid, values_epa, label=\"Epanechnikov\")\n",
    "ax.plot(grid, stats.norm.pdf(grid, loc=mu, scale=sigma), label=\"True density\")\n",
    "# plot histogram for comparison\n",
    "ax.hist(\n",
    "    sample,\n",
    "    bins=grid,\n",
    "    density=True,\n",
    "    histtype=\"step\",\n",
    "    edgecolor=\"black\",\n",
    "    linewidth=0.5,\n",
    "    label=\"Histogram\",\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Regression\n",
    "\n",
    "Context: we want to investigate the nonparametric regression relation $y_i = m(x_i) +\n",
    "\\epsilon_i$, where $y_i$ is a dependent variable, $x_i$ an explanatory variable, and\n",
    "$\\epsilon_i$ an iid error term, for observations $i = 1, ..., n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def m(x):\n",
    "    \"\"\"True function.\"\"\"\n",
    "    return 3 * np.sin(x) + 2 * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = np.random.default_rng(seed=28071995).normal(0, sigma / 2, size=n)\n",
    "y = m(sample) + epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nw_estimator(x, y, h, sample, kernel_type):\n",
    "    \"\"\"Nadaraya - Watson / Local constant estimator.\"\"\"\n",
    "    k = np.vectorize(kernels[kernel_type])\n",
    "    k0 = k((x - sample) / h)\n",
    "    numerator = np.sum(k0 * y)\n",
    "    denominator = np.sum(k0)\n",
    "    return numerator / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ll_estimator(x, y, h, sample, kernel_type):\n",
    "    \"\"\"Local linear estimator. See Li & Racine, p. 81.\"\"\"\n",
    "    k = np.vectorize(kernels[kernel_type])\n",
    "    k_0 = k((x - sample) / h)\n",
    "    s_2 = np.sum(k_0 * (sample - x) ** 2)\n",
    "    s_1 = np.sum(k_0 * (sample - x))\n",
    "    w = k_0 * (s_2 - s_1 * (sample - x))\n",
    "    numerator = np.sum(w * y)\n",
    "    denominator = np.sum(w)\n",
    "    return numerator / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ll_estimator2(x, y, h, sample, kernel_type):\n",
    "    \"\"\"Local linear estimator. See Li & Racine, p. 81.\"\"\"\n",
    "    k = np.vectorize(kernels[kernel_type])\n",
    "    w = np.diag(k((sample - x) / h))\n",
    "    z = np.array((np.ones(len(sample)), sample - x))\n",
    "    return np.linalg.inv(z.dot(w).dot(z.transpose())).dot(z).dot(w).dot(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loocv_error(h, y, sample, kernel_type):\n",
    "    \"\"\"Compute the LOOCV error for a given bandwidth.\"\"\"\n",
    "    error_nw = 0\n",
    "    error_ll = 0\n",
    "    estimator_nw = partial(nw_estimator, h=h, sample=sample, kernel_type=kernel_type)\n",
    "    estimator_ll = partial(ll_estimator, h=h, sample=sample, kernel_type=kernel_type)\n",
    "\n",
    "    # For each observation\n",
    "    for i in range(len(sample)):\n",
    "        # Create a new sample excluding the current observation\n",
    "        sample_loo = np.delete(sample, i)\n",
    "        y_loo = np.delete(y, i)\n",
    "\n",
    "        # Update the sample and y in the estimator\n",
    "        estimator_nw.keywords[\"sample\"] = sample_loo\n",
    "        estimator_nw.keywords[\"y\"] = y_loo\n",
    "        estimator_ll.keywords[\"sample\"] = sample_loo\n",
    "        estimator_ll.keywords[\"y\"] = y_loo\n",
    "\n",
    "        # Compute the prediction for the left-out observation\n",
    "        prediction_nw = estimator_nw(x=sample[i])\n",
    "        prediction_ll = estimator_ll(x=sample[i])\n",
    "\n",
    "        # Add the squared error to the total error\n",
    "        if not np.isnan(prediction_nw):\n",
    "            error_nw += (y[i] - prediction_nw) ** 2\n",
    "        if not np.isnan(prediction_ll):\n",
    "            error_ll += (y[i] - prediction_ll) ** 2\n",
    "\n",
    "    # Return the average error\n",
    "    return error_nw / len(sample), error_ll / len(sample)\n",
    "\n",
    "\n",
    "# List of bandwidths to consider\n",
    "h_values = np.linspace(0.2, 2, 100)\n",
    "\n",
    "# Compute the LOOCV error for each bandwidth\n",
    "loocv_part = partial(loocv_error, sample=sample, y=y, kernel_type=\"Epanechnikov\")\n",
    "loocv_vec = np.vectorize(loocv_part)\n",
    "errors = loocv_vec(h_values)\n",
    "\n",
    "# Choose the bandwidth with the smallest error\n",
    "loocv_h_nw, loocv_h_ll = h_values[np.argmin(errors, axis=1)]\n",
    "f\"Optimal bandwidth: {loocv_h_nw, loocv_h_ll}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1 = partial(\n",
    "    nw_estimator,\n",
    "    y=y,\n",
    "    h=loocv_h_nw,\n",
    "    sample=sample,\n",
    "    kernel_type=\"Epanechnikov\",\n",
    ")\n",
    "temp2 = np.vectorize(temp1)\n",
    "temp3 = partial(\n",
    "    ll_estimator,\n",
    "    y=y,\n",
    "    h=loocv_h_ll,\n",
    "    sample=sample,\n",
    "    kernel_type=\"Epanechnikov\",\n",
    ")\n",
    "temp4 = np.vectorize(temp3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(grid, m(grid), label=\"True relation\")\n",
    "ax.plot(grid, temp2(x=grid), label=\"Nadaraya-Watson estimator\")\n",
    "ax.plot(grid, temp4(x=grid), label=\"Local linear estimator\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently the Leave-One-Out Cross Validation algorithm gets stuck in a local optimum with very small bandwidths, at least for the Nadaraya-Watson estimator. I don't understand why, maybe because it can get perfect in sample fit if the observations are not so dense?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functional Data Analysis\n",
    "\n",
    "Ideas for simulation\n",
    "- Uni- vs. Multivariate case\n",
    "- Simulate different normal distributions\n",
    "- Vary parameters of (generalized) Beta distribution, so principal components can be interpreted as varying parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$X(t) = \\sum_{k=1}^n η_k φ_k(t)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation Method Paper (Petersen & Müller 2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equispaced grid on [0, 1]\n",
    "gridnum = 100\n",
    "grid_fda = np.linspace(start=-np.ones(n), stop=np.ones(n), num=gridnum)\n",
    "grid_univ = np.linspace(start=-1, stop=1, num=gridnum)\n",
    "\n",
    "\n",
    "# Define normal density\n",
    "def norm_density(x, mu, sigma):\n",
    "    \"\"\"Define normal density function.\n",
    "\n",
    "    To test: columns of x must align with mu and sigma.\n",
    "    \"\"\"\n",
    "    x = np.array(x)  # to vectorize the input\n",
    "    mu = np.array(mu)\n",
    "    sigma = np.array(sigma)\n",
    "    return np.reciprocal(np.sqrt(2 * np.pi) * sigma) * np.exp(\n",
    "        (-0.5) * ((x - mu) / sigma) ** 2,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw different sigmas\n",
    "log_sigmas = np.random.default_rng(seed=28071995).uniform(-1.5, 1.5, n)\n",
    "mus = np.zeros(n)\n",
    "sigmas = np.exp(log_sigmas)\n",
    "densities_discretized = norm_density(grid_fda, mus, sigmas).transpose()\n",
    "densities_discretized[0], sigmas[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do FPCA via package\n",
    "fpca_discretized = FPCA(n_components=1)\n",
    "fd = skfda.FDataGrid(densities_discretized)\n",
    "fd.dataset_name = \"Density Samples\"\n",
    "fd.argument_names = (\"x-Value\",)\n",
    "fd.coordinate_names = (\"Density\",)\n",
    "fpca_discretized.fit(fd)\n",
    "fpca_discretized.components_.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample densities\n",
    "partial_vectorized = np.vectorize(partial)\n",
    "densities = partial_vectorized(norm_density, mu=mus, sigma=sigmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual computation of FPCs\n",
    "# Sample Mean:\n",
    "def sample_mean(x: float, sample_funcs: np.ndarray) -> float:\n",
    "    \"\"\"Compute mean function.\"\"\"\n",
    "    sum_of_funcs = 0\n",
    "    for f in sample_funcs:\n",
    "        sum_of_funcs += f(x)\n",
    "    return 1 / len(sample_funcs) * sum_of_funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_cov_func(x: float, y: float, mean_func: callable, sample_funcs: np.ndarray):\n",
    "    \"\"\"Compute covariance function.\"\"\"\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    mean_x = mean_func(x, sample_funcs)\n",
    "    mean_y = mean_func(y, sample_funcs)\n",
    "    sum_cross_products = 0\n",
    "    sum_x_evals = 0\n",
    "    sum_y_evals = 0\n",
    "    for f in sample_funcs:\n",
    "        sum_cross_products += f(x) * f(y)\n",
    "        sum_x_evals += f(x)\n",
    "        sum_y_evals += f(y)\n",
    "    return mean_x * mean_y + 1 / len(sample_funcs) * (\n",
    "        sum_cross_products - mean_x * sum_y_evals - mean_y * sum_x_evals\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_cov_func2(x: float, y: float, mean_func: callable, sample_funcs: np.ndarray):\n",
    "    \"\"\"Compute covariance function.\"\"\"\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    mean_x = mean_func(x, sample_funcs)\n",
    "    mean_y = mean_func(y, sample_funcs)\n",
    "    sum_products = 0\n",
    "    for f in sample_funcs:\n",
    "        sum_products += (f(x) - mean_x) * (f(y) - mean_y)\n",
    "    return 1 / len(sample_funcs) * sum_products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See whether they agree on the the result, they should\n",
    "sample_cov_func(0, 0.5, sample_mean, densities), sample_cov_func2(\n",
    "    0,\n",
    "    0.5,\n",
    "    sample_mean,\n",
    "    densities,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "approx_cov_func = np.zeros((gridnum, gridnum))\n",
    "for i in range(gridnum):\n",
    "    approx_cov_func[i] = sample_cov_func(\n",
    "        grid_univ,\n",
    "        grid_univ[i],\n",
    "        sample_mean,\n",
    "        densities,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "approx_cov_func2 = np.zeros((gridnum, gridnum))\n",
    "for i in range(gridnum):\n",
    "    approx_cov_func2[i] = sample_cov_func2(\n",
    "        grid_univ,\n",
    "        grid_univ[i],\n",
    "        sample_mean,\n",
    "        densities,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "approx_cov_np = np.cov(densities_discretized.transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy command obviously much faster, so won't use my own functions.\n",
    "\n",
    "Now,\n",
    "\n",
    "## Estimation of Eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute covariance matrix\n",
    "approx_cov_np = np.cov(densities_discretized.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eig_vals, eig_vecs = np.linalg.eigh(approx_cov_np)\n",
    "eig_vals2, eig_vecs2 = np.linalg.eigh(approx_cov_np)\n",
    "eig_vals_sorted = eig_vals[np.argsort(-eig_vals)]\n",
    "eig_vecs_sorted = eig_vecs[:, np.argsort(-eig_vals)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = eig_vecs_sorted[:, 0] - eig_vecs[:, np.argmax(-eig_vals)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\int_0^1 f(t)dt$  wird dann durch die Riemann Summe $1/m \\sum_{j=1}^m f(s_j)$ ersetzt ($s_j$  - Gridpunkte, $m$ -  Anzahl der Gridpunkte)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def riemann_sum(a, b, m, f, method=\"left\"):\n",
    "    \"\"\"Compute integral.\"\"\"\n",
    "    stepsize = (b - a) / m\n",
    "    if method == \"left\":\n",
    "        grid = np.linspace(a, b - stepsize, m)\n",
    "    elif method == \"right\":\n",
    "        grid = np.linspace(a + stepsize, b, m)\n",
    "    else:\n",
    "        msg = \"Must specify either left or right Riemann sum!\"\n",
    "        raise ValueError(msg)\n",
    "    return sum(f(grid) * stepsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def riemann_sum2(vector):\n",
    "    \"\"\"Compute integral.\"\"\"\n",
    "    return sum(vector * 1 / len(vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "riemann_sum(vector=eig_vecs_sorted[:, 0] ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Generate synthetic functional data\n",
    "n_curves = 100\n",
    "n_points = 100\n",
    "x = np.linspace(0, 1, n_points)\n",
    "# Generate curves with some random variation around a sine curve\n",
    "data = np.array(\n",
    "    [\n",
    "        np.sin(4 * np.pi * x) + 0.5 * np.random.default_rng(_).normal(0, 0.5, len(x))\n",
    "        for _ in range(n_curves)\n",
    "    ],\n",
    ")\n",
    "\n",
    "# 2. Compute the mean function\n",
    "mean_function = np.mean(data, axis=0)\n",
    "\n",
    "# 3. Center the data\n",
    "centered_data = data - mean_function\n",
    "\n",
    "# 4. Estimate the covariance function using a discrete approximation\n",
    "cov_matrix = np.cov(centered_data, rowvar=False)\n",
    "\n",
    "# 5. Compute the eigenfunctions (principal components) of the covariance matrix\n",
    "eigenvalues, eigenfunctions = np.linalg.eigh(cov_matrix)\n",
    "\n",
    "# Sort eigenvalues and eigenfunctions in decreasing order\n",
    "eigenvalues = eigenvalues[::-1]\n",
    "eigenfunctions = eigenfunctions[:, ::-1]\n",
    "\n",
    "# 6. Project the centered data onto the eigenfunctions\n",
    "fpca_scores = np.dot(centered_data, eigenfunctions)\n",
    "\n",
    "# Plot the mean function and the first two eigenfunctions\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(x, mean_function, \"b-\")\n",
    "plt.title(\"Mean Function\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(x, eigenfunctions[:, 0], \"r-\")\n",
    "plt.title(\"1st Eigenfunction\")\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(x, eigenfunctions[:, 1], \"g-\")\n",
    "plt.title(\"2nd Eigenfunction\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(eigenfunctions[:, 0])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
