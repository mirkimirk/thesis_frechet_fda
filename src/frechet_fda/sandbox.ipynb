{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Sandbox module.\"\"\"\n",
    "from functools import partial\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import skfda\n",
    "from misc import (\n",
    "    cdf_estimator,\n",
    "    kernel_estimator,\n",
    "    norm_density,\n",
    "    quantile_estimator,\n",
    ")\n",
    "from scipy.stats import norm\n",
    "from skfda.preprocessing.dim_reduction import FPCA\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\int_0^1 f(t)dt$  wird dann durch die Riemann Summe $1/m \\sum_{j=1}^m f(s_j)$ ersetzt ($s_j$  - Gridpunkte, $m$ -  Anzahl der Gridpunkte)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nonparametric Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel density estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To implement: Adjusted estimator from Müller and Petersen 2016, and compact support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix parameters and generate sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "n_grid = 200\n",
    "grid_ending = 40\n",
    "mu = 0\n",
    "sigma = 10\n",
    "\n",
    "sample = np.random.default_rng(seed=28071995).normal(loc=mu, scale=sigma, size=n)\n",
    "grid = np.linspace(start=-grid_ending, stop=grid_ending, num=n_grid)\n",
    "# Rule-of-Thumb bandwidth (Li and Racine 2007, p. 66)\n",
    "bandwidth = np.std(sample) * (n ** (-0.2))  # should implement optimal bandwidth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_estimator_given_sample = partial(kernel_estimator, sample=sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate fitted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_epa = kernel_estimator_given_sample(\n",
    "    x=grid,\n",
    "    h=bandwidth,\n",
    "    kernel_type=\"epanechnikov\",\n",
    ")\n",
    "values_uni = kernel_estimator_given_sample(x=grid, h=bandwidth, kernel_type=\"uniform\")\n",
    "values_tri = kernel_estimator_given_sample(\n",
    "    x=grid,\n",
    "    h=bandwidth,\n",
    "    kernel_type=\"triangular\",\n",
    ")\n",
    "values_cdf = cdf_estimator(x=grid, f=kernel_estimator, h=bandwidth, sample=sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(grid, norm.pdf(grid, loc=mu, scale=sigma), label=\"True density\")\n",
    "ax.plot(grid, values_epa, label=\"Epanechnikov\")\n",
    "ax.plot(grid, values_uni, label=\"Uniform\")\n",
    "ax.plot(grid, values_tri, label=\"Triangular\")\n",
    "# plot histogram for comparison\n",
    "ax.hist(\n",
    "    sample,\n",
    "    bins=grid,\n",
    "    density=True,\n",
    "    histtype=\"step\",\n",
    "    edgecolor=\"black\",\n",
    "    linewidth=0.5,\n",
    "    label=\"Histogram\",\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(grid, values_cdf, label=\"Epanechnikov\")\n",
    "ax.plot(grid, norm.pdf(grid, loc=mu, scale=sigma), label=\"True density\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Regression\n",
    "\n",
    "Context: we want to investigate the nonparametric regression relation $y_i = m(x_i) +\n",
    "\\epsilon_i$, where $y_i$ is a dependent variable, $x_i$ an explanatory variable, and\n",
    "$\\epsilon_i$ an iid error term, for observations $i = 1, ..., n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def m(x):\n",
    "    \"\"\"True function.\"\"\"\n",
    "    return 3 * np.sin(x) + 2 * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = np.random.default_rng(seed=28071995).normal(0, sigma / 2, size=n)\n",
    "y = m(sample) + epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nw_estimator(x, y, h, sample, kernel_type):\n",
    "    \"\"\"Nadaraya - Watson / Local constant estimator.\"\"\"\n",
    "    k = np.vectorize(kernels[kernel_type])\n",
    "    k0 = k((x - sample) / h)\n",
    "    numerator = np.sum(k0 * y)\n",
    "    denominator = np.sum(k0)\n",
    "    return numerator / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ll_estimator(x, y, h, sample, kernel_type):\n",
    "    \"\"\"Local linear estimator. See Li & Racine, p. 81.\"\"\"\n",
    "    k = np.vectorize(kernels[kernel_type])\n",
    "    k_0 = k((x - sample) / h)\n",
    "    s_2 = np.sum(k_0 * (sample - x) ** 2)\n",
    "    s_1 = np.sum(k_0 * (sample - x))\n",
    "    w = k_0 * (s_2 - s_1 * (sample - x))\n",
    "    numerator = np.sum(w * y)\n",
    "    denominator = np.sum(w)\n",
    "    return numerator / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ll_estimator2(x, y, h, sample, kernel_type):\n",
    "    \"\"\"Local linear estimator. See Li & Racine, p. 81.\"\"\"\n",
    "    k = np.vectorize(kernels[kernel_type])\n",
    "    w = np.diag(k((sample - x) / h))\n",
    "    z = np.array((np.ones(len(sample)), sample - x))\n",
    "    return np.linalg.inv(z.dot(w).dot(z.transpose())).dot(z).dot(w).dot(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels = {\n",
    "    \"epanechnikov\": lambda u: 3\n",
    "    / (4 * np.sqrt(5))\n",
    "    * (1 - (u**2) / 5)\n",
    "    * (np.abs(u) <= np.sqrt(5)),\n",
    "    \"uniform\": lambda u: 0.5 * (np.abs(u) <= 1),\n",
    "    \"triangular\": lambda u: (1 - np.abs(u)) * (np.abs(u) <= 1),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loocv_error(h, y, sample, kernel_type):\n",
    "    \"\"\"Compute the LOOCV error for a given bandwidth.\"\"\"\n",
    "    error_nw = 0\n",
    "    error_ll = 0\n",
    "    estimator_nw = partial(nw_estimator, h=h, sample=sample, kernel_type=kernel_type)\n",
    "    estimator_ll = partial(ll_estimator, h=h, sample=sample, kernel_type=kernel_type)\n",
    "\n",
    "    # For each observation\n",
    "    for i in range(len(sample)):\n",
    "        # Create a new sample excluding the current observation\n",
    "        sample_loo = np.delete(sample, i)\n",
    "        y_loo = np.delete(y, i)\n",
    "\n",
    "        # Update the sample and y in the estimator\n",
    "        estimator_nw.keywords[\"sample\"] = sample_loo\n",
    "        estimator_nw.keywords[\"y\"] = y_loo\n",
    "        estimator_ll.keywords[\"sample\"] = sample_loo\n",
    "        estimator_ll.keywords[\"y\"] = y_loo\n",
    "\n",
    "        # Compute the prediction for the left-out observation\n",
    "        prediction_nw = estimator_nw(x=sample[i])\n",
    "        prediction_ll = estimator_ll(x=sample[i])\n",
    "\n",
    "        # Add the squared error to the total error\n",
    "        if not np.isnan(prediction_nw):\n",
    "            error_nw += (y[i] - prediction_nw) ** 2\n",
    "        if not np.isnan(prediction_ll):\n",
    "            error_ll += (y[i] - prediction_ll) ** 2\n",
    "\n",
    "    # Return the average error\n",
    "    return error_nw / len(sample), error_ll / len(sample)\n",
    "\n",
    "\n",
    "# List of bandwidths to consider\n",
    "h_values = np.linspace(0.5, 2, 100)\n",
    "\n",
    "# Compute the LOOCV error for each bandwidth\n",
    "loocv_part = partial(loocv_error, sample=sample, y=y, kernel_type=\"epanechnikov\")\n",
    "loocv_vec = np.vectorize(loocv_part)\n",
    "errors = loocv_vec(h_values)\n",
    "\n",
    "# Choose the bandwidth with the smallest error\n",
    "loocv_h_nw, loocv_h_ll = h_values[np.argmin(errors, axis=1)]\n",
    "f\"Optimal bandwidth: {loocv_h_nw, loocv_h_ll}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1 = partial(\n",
    "    nw_estimator,\n",
    "    y=y,\n",
    "    h=loocv_h_nw,\n",
    "    sample=sample,\n",
    "    kernel_type=\"epanechnikov\",\n",
    ")\n",
    "temp2 = np.vectorize(temp1)\n",
    "temp3 = partial(\n",
    "    ll_estimator,\n",
    "    y=y,\n",
    "    h=loocv_h_ll,\n",
    "    sample=sample,\n",
    "    kernel_type=\"epanechnikov\",\n",
    ")\n",
    "temp4 = np.vectorize(temp3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(grid, m(grid), label=\"True relation\")\n",
    "ax.plot(grid, temp2(x=grid), label=\"Nadaraya-Watson estimator\")\n",
    "ax.plot(grid, temp4(x=grid), label=\"Local linear estimator\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently the Leave-One-Out Cross Validation algorithm gets stuck in a local optimum with very small bandwidths, at least for the Nadaraya-Watson estimator. I don't understand why, maybe because it can get perfect in sample fit if the observations are not so dense?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functional Data Analysis\n",
    "\n",
    "Ideas for simulation\n",
    "- Uni- vs. Multivariate case\n",
    "- Simulate different normal distributions\n",
    "- Vary parameters of (generalized) Beta distribution, so principal components can be interpreted as varying parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$X(t) = \\sum_{k=1}^n η_k φ_k(t)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation Method Paper (Petersen & Müller 2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equispaced grid on [0, 1]\n",
    "n = 200\n",
    "gridnum = 100\n",
    "grid_densities_univ = np.linspace(start=-5, stop=5, num=gridnum)\n",
    "grid_quantiles_univ = np.linspace(start=0, stop=1, num=gridnum)\n",
    "# Draw different sigmas\n",
    "grid_densities = np.linspace(start=-np.ones(n), stop=np.ones(n), num=gridnum)\n",
    "grid_quantiles = np.linspace(\n",
    "    start=np.ones(n) * 0.01,\n",
    "    stop=np.ones(n) * 0.99,\n",
    "    num=gridnum,\n",
    ")\n",
    "log_sigmas = np.random.default_rng(seed=28071995).uniform(-1.5, 1.5, n)\n",
    "mus = np.zeros(n)\n",
    "sigmas = np.exp(log_sigmas)\n",
    "densities_discretized = norm_density(grid_densities, mus, sigmas).transpose()\n",
    "quantiles_discretized = norm.ppf(grid_quantiles, mus, sigmas).transpose()\n",
    "quantile_densities_discretized = np.reciprocal(\n",
    "    norm_density(norm.ppf(grid_quantiles, mus, sigmas), mus, sigmas),\n",
    ").transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile_estimator([0.5, 0.97], kernel_estimator, 0.2, sample, grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do FPCA via package\n",
    "fpca_discretized = FPCA(n_components=2)\n",
    "fd = skfda.FDataGrid(\n",
    "    data_matrix=densities_discretized,\n",
    "    grid_points=grid_densities_univ,\n",
    "    dataset_name=\"Density Samples\",\n",
    "    argument_names=(\"x-Value\",),\n",
    "    coordinate_names=(\"Density\",),\n",
    ")\n",
    "fpca_discretized.fit(fd)\n",
    "fpca_discretized.components_.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample densities\n",
    "partial_vectorized = np.vectorize(partial)\n",
    "densities = partial_vectorized(norm_density, mu=mus, sigma=sigmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual computation of FPCs, but bad! Better: replace it with np.mean of\n",
    "# densities_discretized\n",
    "# Sample Mean:\n",
    "def sample_mean(x: float, sample_funcs: np.ndarray) -> float:\n",
    "    \"\"\"Compute mean function.\"\"\"\n",
    "    sum_of_funcs = 0\n",
    "    for f in sample_funcs:\n",
    "        sum_of_funcs += f(x)\n",
    "    return 1 / len(sample_funcs) * sum_of_funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_cov_func(x: float, y: float, mean_func: callable, sample_funcs: np.ndarray):\n",
    "    \"\"\"Compute covariance function.\"\"\"\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    mean_x = mean_func(x, sample_funcs)\n",
    "    mean_y = mean_func(y, sample_funcs)\n",
    "    sum_cross_products = 0\n",
    "    sum_x_evals = 0\n",
    "    sum_y_evals = 0\n",
    "    for f in sample_funcs:\n",
    "        sum_cross_products += f(x) * f(y)\n",
    "        sum_x_evals += f(x)\n",
    "        sum_y_evals += f(y)\n",
    "    return mean_x * mean_y + 1 / len(sample_funcs) * (\n",
    "        sum_cross_products - mean_x * sum_y_evals - mean_y * sum_x_evals\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_cov_func2(x: float, y: float, mean_func: callable, sample_funcs: np.ndarray):\n",
    "    \"\"\"Compute covariance function.\"\"\"\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    mean_x = mean_func(x, sample_funcs)\n",
    "    mean_y = mean_func(y, sample_funcs)\n",
    "    sum_products = 0\n",
    "    for f in sample_funcs:\n",
    "        sum_products += (f(x) - mean_x) * (f(y) - mean_y)\n",
    "    return 1 / len(sample_funcs) * sum_products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See whether they agree on the the result, they should\n",
    "sample_cov_func(0, 0.5, sample_mean, densities), sample_cov_func2(\n",
    "    0,\n",
    "    0.5,\n",
    "    sample_mean,\n",
    "    densities,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "approx_cov_func = np.zeros((gridnum, gridnum))\n",
    "for i in range(gridnum):\n",
    "    approx_cov_func[i] = sample_cov_func(\n",
    "        grid_densities_univ,\n",
    "        grid_densities_univ[i],\n",
    "        sample_mean,\n",
    "        densities,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "approx_cov_func2 = np.zeros((gridnum, gridnum))\n",
    "for i in range(gridnum):\n",
    "    approx_cov_func2[i] = sample_cov_func2(\n",
    "        grid_densities_univ,\n",
    "        grid_densities_univ[i],\n",
    "        sample_mean,\n",
    "        densities,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "approx_cov_np = np.cov(densities_discretized.transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy command obviously much faster, so won't use my own functions."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
